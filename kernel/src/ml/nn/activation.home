// HomeOS Neural Network Activation Functions
// Non-linear activation layers

const basics = @import("basics")
const tensor = @import("ml/tensor")
const tensor_ops = @import("ml/tensor_ops")
const autograd = @import("ml/autograd")
const module = @import("ml/nn/module")

// ============================================
// ReLU
// ============================================

fn relu_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    return autograd.autograd_relu(input)
}

export fn ReLU(): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "ReLU", module.MODULE_RELU)
    m.forward_fn = relu_forward
    return m
}

// Functional ReLU
export fn relu(input: *tensor.Tensor): *tensor.Tensor {
    return autograd.autograd_relu(input)
}

// In-place ReLU
export fn relu_(input: *tensor.Tensor): *tensor.Tensor {
    if input == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let data = input.data as *f32
        var i: u64 = 0
        while i < numel {
            if data[i] < 0.0 {
                data[i] = 0.0
            }
            i = i + 1
        }
    }

    return input
}

// ============================================
// Leaky ReLU
// ============================================

struct LeakyReLUData {
    negative_slope: f32
}

fn leaky_relu_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    let extra = m.extra as *LeakyReLUData
    return leaky_relu(input, extra.negative_slope as f64)
}

export fn LeakyReLU(negative_slope: f64): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "LeakyReLU", module.MODULE_RELU)

    let extra = basics.alloc(LeakyReLUData) as *LeakyReLUData
    extra.negative_slope = negative_slope as f32
    m.extra = extra as *void

    m.forward_fn = leaky_relu_forward
    return m
}

export fn leaky_relu(input: *tensor.Tensor, negative_slope: f64): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        let slope = negative_slope as f32
        var i: u64 = 0
        while i < numel {
            ro[i] = if ri[i] > 0.0 { ri[i] } else { ri[i] * slope }
            i = i + 1
        }
    }

    return result
}

// ============================================
// ELU (Exponential Linear Unit)
// ============================================

struct ELUData {
    alpha: f32
}

fn elu_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    let extra = m.extra as *ELUData
    return elu(input, extra.alpha as f64)
}

export fn ELU(alpha: f64): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "ELU", module.MODULE_RELU)

    let extra = basics.alloc(ELUData) as *ELUData
    extra.alpha = alpha as f32
    m.extra = extra as *void

    m.forward_fn = elu_forward
    return m
}

export fn elu(input: *tensor.Tensor, alpha: f64): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        let a = alpha as f32
        var i: u64 = 0
        while i < numel {
            if ri[i] > 0.0 {
                ro[i] = ri[i]
            } else {
                ro[i] = a * (basics.exp(ri[i] as f64) - 1.0) as f32
            }
            i = i + 1
        }
    }

    return result
}

// ============================================
// GELU (Gaussian Error Linear Unit)
// ============================================

fn gelu_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    return gelu(input)
}

export fn GELU(): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "GELU", module.MODULE_RELU)
    m.forward_fn = gelu_forward
    return m
}

// GELU approximation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
export fn gelu(input: *tensor.Tensor): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)
    let sqrt_2_pi: f64 = 0.7978845608028654  // sqrt(2/pi)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        var i: u64 = 0
        while i < numel {
            let x = ri[i] as f64
            let inner = sqrt_2_pi * (x + 0.044715 * x * x * x)
            ro[i] = (0.5 * x * (1.0 + basics.tanh(inner))) as f32
            i = i + 1
        }
    }

    return result
}

// ============================================
// Sigmoid
// ============================================

fn sigmoid_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    return autograd.autograd_sigmoid(input)
}

export fn Sigmoid(): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "Sigmoid", module.MODULE_SIGMOID)
    m.forward_fn = sigmoid_forward
    return m
}

export fn sigmoid(input: *tensor.Tensor): *tensor.Tensor {
    return tensor_ops.tensor_sigmoid(input)
}

// ============================================
// Tanh
// ============================================

fn tanh_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    return autograd.autograd_tanh(input)
}

export fn Tanh(): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "Tanh", module.MODULE_TANH)
    m.forward_fn = tanh_forward
    return m
}

export fn tanh(input: *tensor.Tensor): *tensor.Tensor {
    return tensor_ops.tensor_tanh(input)
}

// ============================================
// Softmax
// ============================================

struct SoftmaxData {
    dim: i32
}

fn softmax_forward(m: *module.Module, input: *tensor.Tensor): *tensor.Tensor {
    let extra = m.extra as *SoftmaxData
    return softmax(input, extra.dim)
}

export fn Softmax(dim: i32): *module.Module {
    let m = basics.alloc(module.Module) as *module.Module
    if m == null {
        return null
    }
    module.module_init(m, "Softmax", module.MODULE_SOFTMAX)

    let extra = basics.alloc(SoftmaxData) as *SoftmaxData
    extra.dim = dim
    m.extra = extra as *void

    m.forward_fn = softmax_forward
    return m
}

// Softmax along dimension
export fn softmax(input: *tensor.Tensor, dim: i32): *tensor.Tensor {
    if input == null {
        return null
    }

    // Handle negative dim
    var actual_dim = dim
    if actual_dim < 0 {
        actual_dim = actual_dim + (input.ndim as i32)
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    // For now, implement softmax over last dimension for 2D tensor
    if input.ndim == 2 and actual_dim == 1 {
        let batch = input.shape[0]
        let features = input.shape[1]

        if input.dtype == tensor.DTYPE_F32 {
            let ri = input.data as *f32
            let ro = result.data as *f32

            var b: u64 = 0
            while b < batch {
                // Find max for numerical stability
                var max_val: f32 = ri[b * features]
                var f: u64 = 1
                while f < features {
                    if ri[b * features + f] > max_val {
                        max_val = ri[b * features + f]
                    }
                    f = f + 1
                }

                // Compute exp and sum
                var sum: f32 = 0.0
                f = 0
                while f < features {
                    ro[b * features + f] = basics.exp((ri[b * features + f] - max_val) as f64) as f32
                    sum = sum + ro[b * features + f]
                    f = f + 1
                }

                // Normalize
                f = 0
                while f < features {
                    ro[b * features + f] = ro[b * features + f] / sum
                    f = f + 1
                }

                b = b + 1
            }
        }
    } else if input.ndim == 1 {
        // 1D softmax
        let n = input.shape[0]

        if input.dtype == tensor.DTYPE_F32 {
            let ri = input.data as *f32
            let ro = result.data as *f32

            // Find max
            var max_val: f32 = ri[0]
            var i: u64 = 1
            while i < n {
                if ri[i] > max_val {
                    max_val = ri[i]
                }
                i = i + 1
            }

            // Exp and sum
            var sum: f32 = 0.0
            i = 0
            while i < n {
                ro[i] = basics.exp((ri[i] - max_val) as f64) as f32
                sum = sum + ro[i]
                i = i + 1
            }

            // Normalize
            i = 0
            while i < n {
                ro[i] = ro[i] / sum
                i = i + 1
            }
        }
    }

    return result
}

// Log Softmax
export fn log_softmax(input: *tensor.Tensor, dim: i32): *tensor.Tensor {
    let sm = softmax(input, dim)
    if sm == null {
        return null
    }

    let result = tensor_ops.tensor_log(sm)
    tensor.tensor_release(sm)
    return result
}

// ============================================
// Softplus
// ============================================

export fn softplus(input: *tensor.Tensor, beta: f64, threshold: f64): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        var i: u64 = 0
        while i < numel {
            let x = ri[i] as f64
            if x * beta > threshold {
                ro[i] = ri[i]
            } else {
                ro[i] = (basics.log(1.0 + basics.exp(beta * x)) / beta) as f32
            }
            i = i + 1
        }
    }

    return result
}

// ============================================
// Mish
// ============================================

// Mish: x * tanh(softplus(x))
export fn mish(input: *tensor.Tensor): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        var i: u64 = 0
        while i < numel {
            let x = ri[i] as f64
            let sp = basics.log(1.0 + basics.exp(x))
            ro[i] = (x * basics.tanh(sp)) as f32
            i = i + 1
        }
    }

    return result
}

// ============================================
// SiLU / Swish
// ============================================

// SiLU: x * sigmoid(x)
export fn silu(input: *tensor.Tensor): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        var i: u64 = 0
        while i < numel {
            let x = ri[i] as f64
            let sig = 1.0 / (1.0 + basics.exp(-x))
            ro[i] = (x * sig) as f32
            i = i + 1
        }
    }

    return result
}

// Alias
export fn swish(input: *tensor.Tensor): *tensor.Tensor {
    return silu(input)
}

// ============================================
// Hardswish
// ============================================

export fn hardswish(input: *tensor.Tensor): *tensor.Tensor {
    if input == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let ro = result.data as *f32
        var i: u64 = 0
        while i < numel {
            let x = ri[i]
            if x <= -3.0 {
                ro[i] = 0.0
            } else if x >= 3.0 {
                ro[i] = x
            } else {
                ro[i] = x * (x + 3.0) / 6.0
            }
            i = i + 1
        }
    }

    return result
}
