// HomeOS Neural Network Loss Functions
// Loss functions for training

const basics = @import("basics")
const tensor = @import("ml/tensor")
const tensor_ops = @import("ml/tensor_ops")
const activation = @import("ml/nn/activation")

// Reduction modes
const REDUCE_NONE: u32 = 0
const REDUCE_MEAN: u32 = 1
const REDUCE_SUM: u32 = 2

// ============================================
// Mean Squared Error Loss
// ============================================

// MSE: mean((input - target)^2)
export fn mse_loss(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    // Compute (input - target)^2
    let diff = tensor_ops.tensor_sub(input, target)
    if diff == null {
        return null
    }

    let squared = tensor_ops.tensor_mul(diff, diff)
    tensor.tensor_release(diff)
    if squared == null {
        return null
    }

    if reduction == REDUCE_NONE {
        return squared
    }

    // Reduce
    let sum = tensor_ops.tensor_sum_all(squared)
    tensor.tensor_release(squared)

    var shape: [1]u64 = [1]
    let result = tensor.tensor_empty(&shape, 1, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ro = result.data as *f32
        if reduction == REDUCE_MEAN {
            ro[0] = (sum / (numel as f64)) as f32
        } else {
            ro[0] = sum as f32
        }
    }

    return result
}

// L1 Loss: mean(|input - target|)
export fn l1_loss(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    let diff = tensor_ops.tensor_sub(input, target)
    if diff == null {
        return null
    }

    let abs_diff = tensor_ops.tensor_abs(diff)
    tensor.tensor_release(diff)
    if abs_diff == null {
        return null
    }

    if reduction == REDUCE_NONE {
        return abs_diff
    }

    let sum = tensor_ops.tensor_sum_all(abs_diff)
    tensor.tensor_release(abs_diff)

    var shape: [1]u64 = [1]
    let result = tensor.tensor_empty(&shape, 1, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ro = result.data as *f32
        if reduction == REDUCE_MEAN {
            ro[0] = (sum / (numel as f64)) as f32
        } else {
            ro[0] = sum as f32
        }
    }

    return result
}

// Smooth L1 Loss (Huber Loss)
export fn smooth_l1_loss(input: *tensor.Tensor, target: *tensor.Tensor, beta: f64, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let rt = target.data as *f32
        let ro = result.data as *f32

        var i: u64 = 0
        while i < numel {
            let diff = ri[i] - rt[i]
            let abs_diff = if diff < 0.0 { -diff } else { diff }

            if abs_diff < (beta as f32) {
                ro[i] = 0.5 * diff * diff / (beta as f32)
            } else {
                ro[i] = abs_diff - 0.5 * (beta as f32)
            }
            i = i + 1
        }
    }

    if reduction == REDUCE_NONE {
        return result
    }

    let sum = tensor_ops.tensor_sum_all(result)
    tensor.tensor_release(result)

    var shape: [1]u64 = [1]
    let final_result = tensor.tensor_empty(&shape, 1, input.dtype)
    if final_result == null {
        return null
    }

    if input.dtype == tensor.DTYPE_F32 {
        let ro = final_result.data as *f32
        if reduction == REDUCE_MEAN {
            ro[0] = (sum / (numel as f64)) as f32
        } else {
            ro[0] = sum as f32
        }
    }

    return final_result
}

// ============================================
// Cross Entropy Loss
// ============================================

// Cross Entropy with logits: -sum(target * log_softmax(input))
export fn cross_entropy_loss(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    // Compute log_softmax
    let log_probs = activation.log_softmax(input, -1)
    if log_probs == null {
        return null
    }

    // For class labels (not one-hot), select the correct class
    // Assuming target contains class indices

    if target.ndim == 1 and input.ndim == 2 {
        // target: (batch,), input: (batch, classes)
        let batch = input.shape[0]
        let classes = input.shape[1]

        var loss_shape: [1]u64 = [batch]
        let losses = tensor.tensor_empty(&loss_shape, 1, input.dtype)
        if losses == null {
            tensor.tensor_release(log_probs)
            return null
        }

        if input.dtype == tensor.DTYPE_F32 {
            let rlp = log_probs.data as *f32
            let rt = target.data as *f32
            let rl = losses.data as *f32

            var b: u64 = 0
            while b < batch {
                let class_idx = (rt[b] as u64) % classes
                rl[b] = -rlp[b * classes + class_idx]
                b = b + 1
            }
        }

        tensor.tensor_release(log_probs)

        if reduction == REDUCE_NONE {
            return losses
        }

        let sum = tensor_ops.tensor_sum_all(losses)
        tensor.tensor_release(losses)

        var shape: [1]u64 = [1]
        let result = tensor.tensor_empty(&shape, 1, input.dtype)
        if result != null {
            if input.dtype == tensor.DTYPE_F32 {
                let ro = result.data as *f32
                if reduction == REDUCE_MEAN {
                    ro[0] = (sum / (batch as f64)) as f32
                } else {
                    ro[0] = sum as f32
                }
            }
        }

        return result
    }

    tensor.tensor_release(log_probs)
    return null
}

// Negative Log Likelihood Loss
export fn nll_loss(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    // input is assumed to be log probabilities
    if input == null or target == null {
        return null
    }

    if target.ndim == 1 and input.ndim == 2 {
        let batch = input.shape[0]
        let classes = input.shape[1]

        var loss_shape: [1]u64 = [batch]
        let losses = tensor.tensor_empty(&loss_shape, 1, input.dtype)
        if losses == null {
            return null
        }

        if input.dtype == tensor.DTYPE_F32 {
            let ri = input.data as *f32
            let rt = target.data as *f32
            let rl = losses.data as *f32

            var b: u64 = 0
            while b < batch {
                let class_idx = (rt[b] as u64) % classes
                rl[b] = -ri[b * classes + class_idx]
                b = b + 1
            }
        }

        if reduction == REDUCE_NONE {
            return losses
        }

        let sum = tensor_ops.tensor_sum_all(losses)
        tensor.tensor_release(losses)

        var shape: [1]u64 = [1]
        let result = tensor.tensor_empty(&shape, 1, input.dtype)
        if result != null {
            if input.dtype == tensor.DTYPE_F32 {
                let ro = result.data as *f32
                if reduction == REDUCE_MEAN {
                    ro[0] = (sum / (batch as f64)) as f32
                } else {
                    ro[0] = sum as f32
                }
            }
        }

        return result
    }

    return null
}

// ============================================
// Binary Cross Entropy
// ============================================

// BCE: -mean(target * log(input) + (1 - target) * log(1 - input))
export fn binary_cross_entropy(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)
    let eps: f64 = 1e-7  // For numerical stability

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let rt = target.data as *f32
        let ro = result.data as *f32

        var i: u64 = 0
        while i < numel {
            let p = ri[i] as f64
            let t = rt[i] as f64
            let p_clipped = if p < eps { eps } else if p > (1.0 - eps) { 1.0 - eps } else { p }
            ro[i] = -(t * basics.log(p_clipped) + (1.0 - t) * basics.log(1.0 - p_clipped)) as f32
            i = i + 1
        }
    }

    if reduction == REDUCE_NONE {
        return result
    }

    let sum = tensor_ops.tensor_sum_all(result)
    tensor.tensor_release(result)

    var shape: [1]u64 = [1]
    let final_result = tensor.tensor_empty(&shape, 1, input.dtype)
    if final_result != null {
        if input.dtype == tensor.DTYPE_F32 {
            let ro = final_result.data as *f32
            if reduction == REDUCE_MEAN {
                ro[0] = (sum / (numel as f64)) as f32
            } else {
                ro[0] = sum as f32
            }
        }
    }

    return final_result
}

// BCE with logits (more numerically stable)
export fn binary_cross_entropy_with_logits(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    // BCE with logits: max(x, 0) - x * t + log(1 + exp(-|x|))
    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let rt = target.data as *f32
        let ro = result.data as *f32

        var i: u64 = 0
        while i < numel {
            let x = ri[i] as f64
            let t = rt[i] as f64
            let max_x = if x > 0.0 { x } else { 0.0 }
            let abs_x = if x < 0.0 { -x } else { x }
            ro[i] = (max_x - x * t + basics.log(1.0 + basics.exp(-abs_x))) as f32
            i = i + 1
        }
    }

    if reduction == REDUCE_NONE {
        return result
    }

    let sum = tensor_ops.tensor_sum_all(result)
    tensor.tensor_release(result)

    var shape: [1]u64 = [1]
    let final_result = tensor.tensor_empty(&shape, 1, input.dtype)
    if final_result != null {
        if input.dtype == tensor.DTYPE_F32 {
            let ro = final_result.data as *f32
            if reduction == REDUCE_MEAN {
                ro[0] = (sum / (numel as f64)) as f32
            } else {
                ro[0] = sum as f32
            }
        }
    }

    return final_result
}

// ============================================
// KL Divergence
// ============================================

// KL(P || Q) = sum(P * log(P/Q))
export fn kl_div_loss(input: *tensor.Tensor, target: *tensor.Tensor, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    // input is expected to be log probabilities
    // target is expected to be probabilities

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let rt = target.data as *f32
        let ro = result.data as *f32

        var i: u64 = 0
        while i < numel {
            let t = rt[i] as f64
            let log_q = ri[i] as f64
            if t > 0.0 {
                ro[i] = (t * (basics.log(t) - log_q)) as f32
            } else {
                ro[i] = 0.0
            }
            i = i + 1
        }
    }

    if reduction == REDUCE_NONE {
        return result
    }

    let sum = tensor_ops.tensor_sum_all(result)
    tensor.tensor_release(result)

    var shape: [1]u64 = [1]
    let final_result = tensor.tensor_empty(&shape, 1, input.dtype)
    if final_result != null {
        if input.dtype == tensor.DTYPE_F32 {
            let ro = final_result.data as *f32
            if reduction == REDUCE_MEAN {
                ro[0] = (sum / (numel as f64)) as f32
            } else {
                ro[0] = sum as f32
            }
        }
    }

    return final_result
}

// ============================================
// Hinge Loss
// ============================================

// Hinge: max(0, margin - y * x)
export fn hinge_loss(input: *tensor.Tensor, target: *tensor.Tensor, margin: f64, reduction: u32): *tensor.Tensor {
    if input == null or target == null {
        return null
    }

    let result = tensor.tensor_empty(&input.shape, input.ndim, input.dtype)
    if result == null {
        return null
    }

    let numel = tensor.tensor_numel(input)

    if input.dtype == tensor.DTYPE_F32 {
        let ri = input.data as *f32
        let rt = target.data as *f32
        let ro = result.data as *f32

        var i: u64 = 0
        while i < numel {
            let val = margin - (rt[i] * ri[i]) as f64
            ro[i] = (if val > 0.0 { val } else { 0.0 }) as f32
            i = i + 1
        }
    }

    if reduction == REDUCE_NONE {
        return result
    }

    let sum = tensor_ops.tensor_sum_all(result)
    tensor.tensor_release(result)

    var shape: [1]u64 = [1]
    let final_result = tensor.tensor_empty(&shape, 1, input.dtype)
    if final_result != null {
        if input.dtype == tensor.DTYPE_F32 {
            let ro = final_result.data as *f32
            if reduction == REDUCE_MEAN {
                ro[0] = (sum / (numel as f64)) as f32
            } else {
                ro[0] = sum as f32
            }
        }
    }

    return final_result
}

// ============================================
// Triplet Margin Loss
// ============================================

// Triplet: max(0, d(a, p) - d(a, n) + margin)
export fn triplet_margin_loss(
    anchor: *tensor.Tensor,
    positive: *tensor.Tensor,
    negative: *tensor.Tensor,
    margin: f64,
    reduction: u32
): *tensor.Tensor {
    if anchor == null or positive == null or negative == null {
        return null
    }

    // Compute distances
    let diff_pos = tensor_ops.tensor_sub(anchor, positive)
    let diff_neg = tensor_ops.tensor_sub(anchor, negative)

    if diff_pos == null or diff_neg == null {
        if diff_pos != null { tensor.tensor_release(diff_pos) }
        if diff_neg != null { tensor.tensor_release(diff_neg) }
        return null
    }

    // L2 norm squared for each sample
    let batch = anchor.shape[0]
    var loss_shape: [1]u64 = [batch]
    let losses = tensor.tensor_empty(&loss_shape, 1, anchor.dtype)

    if losses != null and anchor.dtype == tensor.DTYPE_F32 {
        let rdp = diff_pos.data as *f32
        let rdn = diff_neg.data as *f32
        let rl = losses.data as *f32

        let features = tensor.tensor_numel(anchor) / batch

        var b: u64 = 0
        while b < batch {
            var d_pos: f64 = 0.0
            var d_neg: f64 = 0.0

            var f: u64 = 0
            while f < features {
                let idx = b * features + f
                d_pos = d_pos + (rdp[idx] * rdp[idx]) as f64
                d_neg = d_neg + (rdn[idx] * rdn[idx]) as f64
                f = f + 1
            }

            d_pos = basics.sqrt(d_pos)
            d_neg = basics.sqrt(d_neg)

            let loss = d_pos - d_neg + margin
            rl[b] = (if loss > 0.0 { loss } else { 0.0 }) as f32

            b = b + 1
        }
    }

    tensor.tensor_release(diff_pos)
    tensor.tensor_release(diff_neg)

    if reduction == REDUCE_NONE {
        return losses
    }

    let sum = tensor_ops.tensor_sum_all(losses)
    tensor.tensor_release(losses)

    var shape: [1]u64 = [1]
    let result = tensor.tensor_empty(&shape, 1, anchor.dtype)
    if result != null {
        if anchor.dtype == tensor.DTYPE_F32 {
            let ro = result.data as *f32
            if reduction == REDUCE_MEAN {
                ro[0] = (sum / (batch as f64)) as f32
            } else {
                ro[0] = sum as f32
            }
        }
    }

    return result
}
