// HomeOS Tensor Library
// N-dimensional tensor with automatic memory management

const basics = @import("basics")

// Data types
const DTYPE_F32: u32 = 0
const DTYPE_F64: u32 = 1
const DTYPE_I32: u32 = 2
const DTYPE_I64: u32 = 3
const DTYPE_U32: u32 = 4
const DTYPE_U64: u32 = 5
const DTYPE_BOOL: u32 = 6

// Device types
const DEVICE_CPU: u32 = 0
const DEVICE_GPU: u32 = 1

// Maximum dimensions
const MAX_DIMS: u32 = 8

// Tensor flags
const TENSOR_CONTIGUOUS: u32 = 1
const TENSOR_REQUIRES_GRAD: u32 = 2
const TENSOR_IS_LEAF: u32 = 4
const TENSOR_GRAD_COMPUTED: u32 = 8

// Tensor structure
struct Tensor {
    // Data storage
    data: *void
    data_size: u64          // Size in bytes

    // Shape and strides
    shape: [u64; MAX_DIMS]
    strides: [u64; MAX_DIMS]
    ndim: u32

    // Type info
    dtype: u32
    device: u32

    // Autograd support
    requires_grad: bool
    grad: *Tensor           // Gradient tensor
    grad_fn: *GradFn        // Backward function
    is_leaf: bool

    // Memory management
    ref_count: u32
    flags: u32

    // View support
    base: *Tensor           // Base tensor if this is a view
    offset: u64             // Offset into base data
}

// Gradient function for autograd
struct GradFn {
    op_type: u32
    inputs: [*Tensor; 4]
    saved_tensors: [*Tensor; 4]
    next_functions: [*GradFn; 4]
    num_inputs: u32
    backward_fn: fn(*GradFn, *Tensor) -> void
}

// Get element size for dtype
fn dtype_size(dtype: u32): u64 {
    if dtype == DTYPE_F32 or dtype == DTYPE_I32 or dtype == DTYPE_U32 {
        return 4
    }
    if dtype == DTYPE_F64 or dtype == DTYPE_I64 or dtype == DTYPE_U64 {
        return 8
    }
    if dtype == DTYPE_BOOL {
        return 1
    }
    return 4  // Default
}

// Calculate total elements
fn calc_numel(shape: *u64, ndim: u32): u64 {
    var total: u64 = 1
    var i: u32 = 0
    while i < ndim {
        total = total * shape[i]
        i = i + 1
    }
    return total
}

// Calculate strides for contiguous layout (row-major)
fn calc_strides(shape: *u64, strides: *u64, ndim: u32, dtype: u32): void {
    if ndim == 0 {
        return
    }

    let elem_size = dtype_size(dtype)
    var stride: u64 = elem_size

    var i: i32 = (ndim - 1) as i32
    while i >= 0 {
        strides[i as u32] = stride
        stride = stride * shape[i as u32]
        i = i - 1
    }
}

// Allocate tensor memory
fn tensor_alloc_data(t: *Tensor): bool {
    let numel = calc_numel(&t.shape, t.ndim)
    let elem_size = dtype_size(t.dtype)
    t.data_size = numel * elem_size

    if t.device == DEVICE_CPU {
        t.data = basics.alloc(t.data_size)
    } else {
        // GPU allocation - placeholder
        t.data = basics.alloc(t.data_size)
    }

    return t.data != null
}

// ============================================
// Tensor Creation
// ============================================

// Create empty tensor with given shape
export fn tensor_empty(shape: *u64, ndim: u32, dtype: u32): *Tensor {
    if ndim > MAX_DIMS {
        return null
    }

    let t = basics.alloc(Tensor) as *Tensor
    if t == null {
        return null
    }

    // Initialize
    t.ndim = ndim
    t.dtype = dtype
    t.device = DEVICE_CPU
    t.requires_grad = false
    t.is_leaf = true
    t.grad = null
    t.grad_fn = null
    t.ref_count = 1
    t.flags = TENSOR_CONTIGUOUS | TENSOR_IS_LEAF
    t.base = null
    t.offset = 0

    // Copy shape
    var i: u32 = 0
    while i < ndim {
        t.shape[i] = shape[i]
        i = i + 1
    }

    // Calculate strides
    calc_strides(&t.shape, &t.strides, ndim, dtype)

    // Allocate data
    if !tensor_alloc_data(t) {
        basics.free(t)
        return null
    }

    return t
}

// Create tensor filled with zeros
export fn tensor_zeros(shape: *u64, ndim: u32, dtype: u32): *Tensor {
    let t = tensor_empty(shape, ndim, dtype)
    if t == null {
        return null
    }

    basics.memset(t.data, 0, t.data_size)
    return t
}

// Create tensor filled with ones
export fn tensor_ones(shape: *u64, ndim: u32, dtype: u32): *Tensor {
    let t = tensor_empty(shape, ndim, dtype)
    if t == null {
        return null
    }

    let numel = calc_numel(&t.shape, ndim)

    if dtype == DTYPE_F32 {
        let data = t.data as *f32
        var i: u64 = 0
        while i < numel {
            data[i] = 1.0
            i = i + 1
        }
    } else if dtype == DTYPE_F64 {
        let data = t.data as *f64
        var i: u64 = 0
        while i < numel {
            data[i] = 1.0
            i = i + 1
        }
    } else if dtype == DTYPE_I32 {
        let data = t.data as *i32
        var i: u64 = 0
        while i < numel {
            data[i] = 1
            i = i + 1
        }
    } else if dtype == DTYPE_I64 {
        let data = t.data as *i64
        var i: u64 = 0
        while i < numel {
            data[i] = 1
            i = i + 1
        }
    }

    return t
}

// Create tensor filled with a scalar value
export fn tensor_full(shape: *u64, ndim: u32, value: f64, dtype: u32): *Tensor {
    let t = tensor_empty(shape, ndim, dtype)
    if t == null {
        return null
    }

    let numel = calc_numel(&t.shape, ndim)

    if dtype == DTYPE_F32 {
        let data = t.data as *f32
        var i: u64 = 0
        while i < numel {
            data[i] = value as f32
            i = i + 1
        }
    } else if dtype == DTYPE_F64 {
        let data = t.data as *f64
        var i: u64 = 0
        while i < numel {
            data[i] = value
            i = i + 1
        }
    } else if dtype == DTYPE_I32 {
        let data = t.data as *i32
        var i: u64 = 0
        while i < numel {
            data[i] = value as i32
            i = i + 1
        }
    } else if dtype == DTYPE_I64 {
        let data = t.data as *i64
        var i: u64 = 0
        while i < numel {
            data[i] = value as i64
            i = i + 1
        }
    }

    return t
}

// Simple LCG random number generator state
var rand_state: u64 = 12345

fn rand_next(): u64 {
    rand_state = rand_state * 6364136223846793005 + 1442695040888963407
    return rand_state
}

fn rand_f64(): f64 {
    // Generate random f64 in [0, 1)
    return (rand_next() >> 11) as f64 / 9007199254740992.0
}

// Box-Muller transform for normal distribution
fn randn_pair(out1: *f64, out2: *f64): void {
    let u1 = rand_f64()
    let u2 = rand_f64()

    // Avoid log(0)
    let u1_safe = if u1 < 0.0000001 { 0.0000001 } else { u1 }

    let mag = basics.sqrt(-2.0 * basics.log(u1_safe))
    let two_pi = 6.283185307179586
    *out1 = mag * basics.cos(two_pi * u2)
    *out2 = mag * basics.sin(two_pi * u2)
}

// Create tensor with random uniform values [0, 1)
export fn tensor_rand(shape: *u64, ndim: u32): *Tensor {
    let t = tensor_empty(shape, ndim, DTYPE_F32)
    if t == null {
        return null
    }

    let numel = calc_numel(&t.shape, ndim)
    let data = t.data as *f32

    var i: u64 = 0
    while i < numel {
        data[i] = rand_f64() as f32
        i = i + 1
    }

    return t
}

// Create tensor with random normal values (mean=0, std=1)
export fn tensor_randn(shape: *u64, ndim: u32): *Tensor {
    let t = tensor_empty(shape, ndim, DTYPE_F32)
    if t == null {
        return null
    }

    let numel = calc_numel(&t.shape, ndim)
    let data = t.data as *f32

    var i: u64 = 0
    while i < numel {
        var n1: f64
        var n2: f64
        randn_pair(&n1, &n2)
        data[i] = n1 as f32
        i = i + 1
        if i < numel {
            data[i] = n2 as f32
            i = i + 1
        }
    }

    return t
}

// Create tensor from existing data (copies data)
export fn tensor_from_data(data: *void, shape: *u64, ndim: u32, dtype: u32): *Tensor {
    let t = tensor_empty(shape, ndim, dtype)
    if t == null {
        return null
    }

    basics.memcpy(t.data, data, t.data_size)
    return t
}

// Create identity matrix
export fn tensor_eye(n: u64, dtype: u32): *Tensor {
    var shape: [2]u64 = [n, n]
    let t = tensor_zeros(&shape, 2, dtype)
    if t == null {
        return null
    }

    if dtype == DTYPE_F32 {
        let data = t.data as *f32
        var i: u64 = 0
        while i < n {
            data[i * n + i] = 1.0
            i = i + 1
        }
    } else if dtype == DTYPE_F64 {
        let data = t.data as *f64
        var i: u64 = 0
        while i < n {
            data[i * n + i] = 1.0
            i = i + 1
        }
    }

    return t
}

// Create range tensor
export fn tensor_arange(start: f64, end: f64, step: f64, dtype: u32): *Tensor {
    let n = ((end - start) / step) as u64
    if n == 0 {
        return null
    }

    var shape: [1]u64 = [n]
    let t = tensor_empty(&shape, 1, dtype)
    if t == null {
        return null
    }

    if dtype == DTYPE_F32 {
        let data = t.data as *f32
        var i: u64 = 0
        var val: f64 = start
        while i < n {
            data[i] = val as f32
            val = val + step
            i = i + 1
        }
    } else if dtype == DTYPE_F64 {
        let data = t.data as *f64
        var i: u64 = 0
        var val: f64 = start
        while i < n {
            data[i] = val
            val = val + step
            i = i + 1
        }
    }

    return t
}

// Create linearly spaced tensor
export fn tensor_linspace(start: f64, end: f64, steps: u64, dtype: u32): *Tensor {
    if steps < 2 {
        return null
    }

    var shape: [1]u64 = [steps]
    let t = tensor_empty(&shape, 1, dtype)
    if t == null {
        return null
    }

    let step = (end - start) / ((steps - 1) as f64)

    if dtype == DTYPE_F32 {
        let data = t.data as *f32
        var i: u64 = 0
        while i < steps {
            data[i] = (start + (i as f64) * step) as f32
            i = i + 1
        }
    } else if dtype == DTYPE_F64 {
        let data = t.data as *f64
        var i: u64 = 0
        while i < steps {
            data[i] = start + (i as f64) * step
            i = i + 1
        }
    }

    return t
}

// ============================================
// Memory Management
// ============================================

// Increment reference count
export fn tensor_retain(t: *Tensor): void {
    if t != null {
        t.ref_count = t.ref_count + 1
    }
}

// Decrement reference count and free if zero
export fn tensor_release(t: *Tensor): void {
    if t == null {
        return
    }

    t.ref_count = t.ref_count - 1
    if t.ref_count == 0 {
        // Free gradient if exists
        if t.grad != null {
            tensor_release(t.grad)
        }

        // Free data if not a view
        if t.base == null and t.data != null {
            basics.free(t.data)
        }

        basics.free(t)
    }
}

// Clone tensor (deep copy)
export fn tensor_clone(t: *Tensor): *Tensor {
    if t == null {
        return null
    }

    let clone = tensor_empty(&t.shape, t.ndim, t.dtype)
    if clone == null {
        return null
    }

    // Copy data
    if tensor_is_contiguous(t) {
        basics.memcpy(clone.data, t.data, t.data_size)
    } else {
        // Handle non-contiguous copy
        tensor_copy_data(clone, t)
    }

    clone.requires_grad = t.requires_grad
    clone.device = t.device

    return clone
}

// Detach tensor from computation graph
export fn tensor_detach(t: *Tensor): *Tensor {
    let detached = tensor_clone(t)
    if detached != null {
        detached.requires_grad = false
        detached.grad_fn = null
        detached.is_leaf = true
        detached.flags = detached.flags | TENSOR_IS_LEAF
    }
    return detached
}

// ============================================
// Shape Operations
// ============================================

// Check if tensor is contiguous
export fn tensor_is_contiguous(t: *Tensor): bool {
    return (t.flags & TENSOR_CONTIGUOUS) != 0
}

// Get total number of elements
export fn tensor_numel(t: *Tensor): u64 {
    return calc_numel(&t.shape, t.ndim)
}

// Reshape tensor (returns view if possible)
export fn tensor_reshape(t: *Tensor, new_shape: *u64, ndim: u32): *Tensor {
    if t == null or ndim > MAX_DIMS {
        return null
    }

    // Check that total elements match
    let old_numel = tensor_numel(t)
    let new_numel = calc_numel(new_shape, ndim)
    if old_numel != new_numel {
        return null
    }

    // If contiguous, we can create a view
    if tensor_is_contiguous(t) {
        let view = basics.alloc(Tensor) as *Tensor
        if view == null {
            return null
        }

        // Copy tensor info
        basics.memcpy(view, t, @sizeOf(Tensor))

        // Update shape and strides
        view.ndim = ndim
        var i: u32 = 0
        while i < ndim {
            view.shape[i] = new_shape[i]
            i = i + 1
        }
        calc_strides(&view.shape, &view.strides, ndim, t.dtype)

        // Mark as view
        view.base = t
        view.ref_count = 1
        tensor_retain(t)

        return view
    } else {
        // Need to make contiguous copy first
        let contiguous = tensor_contiguous(t)
        if contiguous == null {
            return null
        }
        let reshaped = tensor_reshape(contiguous, new_shape, ndim)
        if reshaped == null {
            tensor_release(contiguous)
        }
        return reshaped
    }
}

// View tensor with new shape (must be contiguous)
export fn tensor_view(t: *Tensor, new_shape: *u64, ndim: u32): *Tensor {
    if !tensor_is_contiguous(t) {
        return null
    }
    return tensor_reshape(t, new_shape, ndim)
}

// Make tensor contiguous
export fn tensor_contiguous(t: *Tensor): *Tensor {
    if t == null {
        return null
    }

    if tensor_is_contiguous(t) {
        tensor_retain(t)
        return t
    }

    // Create new contiguous tensor
    let cont = tensor_empty(&t.shape, t.ndim, t.dtype)
    if cont == null {
        return null
    }

    // Copy data with proper stride handling
    tensor_copy_data(cont, t)

    cont.requires_grad = t.requires_grad
    cont.device = t.device

    return cont
}

// Copy data handling strides
fn tensor_copy_data(dst: *Tensor, src: *Tensor): void {
    let numel = tensor_numel(src)
    let elem_size = dtype_size(src.dtype)

    // Simple flat copy for now - full stride handling would be more complex
    var i: u64 = 0
    while i < numel {
        let src_offset = tensor_linear_to_offset(src, i)
        let dst_offset = i * elem_size

        basics.memcpy(
            (dst.data as u64 + dst_offset) as *void,
            (src.data as u64 + src_offset) as *void,
            elem_size
        )
        i = i + 1
    }
}

// Convert linear index to byte offset using strides
fn tensor_linear_to_offset(t: *Tensor, linear_idx: u64): u64 {
    var offset: u64 = 0
    var remaining = linear_idx
    var i: u32 = 0

    while i < t.ndim {
        let dim_size = t.shape[i]
        let idx = remaining / dim_size
        remaining = remaining % dim_size
        offset = offset + idx * t.strides[i]
        i = i + 1
    }

    return offset
}

// Squeeze dimension
export fn tensor_squeeze(t: *Tensor, dim: i32): *Tensor {
    if t == null {
        return null
    }

    // Count new dimensions
    var new_ndim: u32 = 0
    var new_shape: [MAX_DIMS]u64
    var new_strides: [MAX_DIMS]u64

    var i: u32 = 0
    while i < t.ndim {
        if dim < 0 {
            // Squeeze all size-1 dimensions
            if t.shape[i] != 1 {
                new_shape[new_ndim] = t.shape[i]
                new_strides[new_ndim] = t.strides[i]
                new_ndim = new_ndim + 1
            }
        } else {
            // Squeeze specific dimension
            if i != (dim as u32) or t.shape[i] != 1 {
                new_shape[new_ndim] = t.shape[i]
                new_strides[new_ndim] = t.strides[i]
                new_ndim = new_ndim + 1
            }
        }
        i = i + 1
    }

    // Create view with new shape
    let view = basics.alloc(Tensor) as *Tensor
    if view == null {
        return null
    }

    basics.memcpy(view, t, @sizeOf(Tensor))
    view.ndim = new_ndim
    basics.memcpy(&view.shape, &new_shape, new_ndim * 8)
    basics.memcpy(&view.strides, &new_strides, new_ndim * 8)
    view.base = t
    view.ref_count = 1
    tensor_retain(t)

    return view
}

// Unsqueeze dimension
export fn tensor_unsqueeze(t: *Tensor, dim: u32): *Tensor {
    if t == null or dim > t.ndim {
        return null
    }

    if t.ndim >= MAX_DIMS {
        return null
    }

    let view = basics.alloc(Tensor) as *Tensor
    if view == null {
        return null
    }

    basics.memcpy(view, t, @sizeOf(Tensor))
    view.ndim = t.ndim + 1

    // Shift dimensions
    var i: u32 = t.ndim
    while i > dim {
        view.shape[i] = view.shape[i - 1]
        view.strides[i] = view.strides[i - 1]
        i = i - 1
    }

    // Insert new dimension
    view.shape[dim] = 1
    view.strides[dim] = if dim < t.ndim { view.strides[dim + 1] * view.shape[dim + 1] } else { dtype_size(t.dtype) }

    view.base = t
    view.ref_count = 1
    tensor_retain(t)

    return view
}

// Transpose two dimensions
export fn tensor_transpose(t: *Tensor, dim0: u32, dim1: u32): *Tensor {
    if t == null or dim0 >= t.ndim or dim1 >= t.ndim {
        return null
    }

    let view = basics.alloc(Tensor) as *Tensor
    if view == null {
        return null
    }

    basics.memcpy(view, t, @sizeOf(Tensor))

    // Swap dimensions
    let tmp_shape = view.shape[dim0]
    view.shape[dim0] = view.shape[dim1]
    view.shape[dim1] = tmp_shape

    let tmp_stride = view.strides[dim0]
    view.strides[dim0] = view.strides[dim1]
    view.strides[dim1] = tmp_stride

    // Transposed tensor is generally not contiguous
    view.flags = view.flags & ~TENSOR_CONTIGUOUS

    view.base = t
    view.ref_count = 1
    tensor_retain(t)

    return view
}

// Permute dimensions
export fn tensor_permute(t: *Tensor, dims: *u32): *Tensor {
    if t == null {
        return null
    }

    let view = basics.alloc(Tensor) as *Tensor
    if view == null {
        return null
    }

    basics.memcpy(view, t, @sizeOf(Tensor))

    // Reorder dimensions
    var i: u32 = 0
    while i < t.ndim {
        view.shape[i] = t.shape[dims[i]]
        view.strides[i] = t.strides[dims[i]]
        i = i + 1
    }

    view.flags = view.flags & ~TENSOR_CONTIGUOUS
    view.base = t
    view.ref_count = 1
    tensor_retain(t)

    return view
}

// Flatten dimensions
export fn tensor_flatten(t: *Tensor, start_dim: u32, end_dim: u32): *Tensor {
    if t == null or start_dim >= t.ndim or end_dim >= t.ndim or start_dim > end_dim {
        return null
    }

    // Calculate flattened size
    var flat_size: u64 = 1
    var i: u32 = start_dim
    while i <= end_dim {
        flat_size = flat_size * t.shape[i]
        i = i + 1
    }

    // Build new shape
    var new_shape: [MAX_DIMS]u64
    var new_ndim: u32 = 0

    i = 0
    while i < start_dim {
        new_shape[new_ndim] = t.shape[i]
        new_ndim = new_ndim + 1
        i = i + 1
    }

    new_shape[new_ndim] = flat_size
    new_ndim = new_ndim + 1

    i = end_dim + 1
    while i < t.ndim {
        new_shape[new_ndim] = t.shape[i]
        new_ndim = new_ndim + 1
        i = i + 1
    }

    return tensor_reshape(t, &new_shape, new_ndim)
}

// ============================================
// Element Access
// ============================================

// Get element at indices (returns as f64)
export fn tensor_get(t: *Tensor, indices: *u64): f64 {
    if t == null {
        return 0.0
    }

    // Calculate offset
    var offset: u64 = 0
    var i: u32 = 0
    while i < t.ndim {
        offset = offset + indices[i] * t.strides[i]
        i = i + 1
    }

    let ptr = (t.data as u64 + offset) as *void

    if t.dtype == DTYPE_F32 {
        return (*(ptr as *f32)) as f64
    } else if t.dtype == DTYPE_F64 {
        return *(ptr as *f64)
    } else if t.dtype == DTYPE_I32 {
        return (*(ptr as *i32)) as f64
    } else if t.dtype == DTYPE_I64 {
        return (*(ptr as *i64)) as f64
    }

    return 0.0
}

// Set element at indices
export fn tensor_set(t: *Tensor, indices: *u64, value: f64): void {
    if t == null {
        return
    }

    // Calculate offset
    var offset: u64 = 0
    var i: u32 = 0
    while i < t.ndim {
        offset = offset + indices[i] * t.strides[i]
        i = i + 1
    }

    let ptr = (t.data as u64 + offset) as *void

    if t.dtype == DTYPE_F32 {
        *(ptr as *f32) = value as f32
    } else if t.dtype == DTYPE_F64 {
        *(ptr as *f64) = value
    } else if t.dtype == DTYPE_I32 {
        *(ptr as *i32) = value as i32
    } else if t.dtype == DTYPE_I64 {
        *(ptr as *i64) = value as i64
    }
}

// ============================================
// Autograd Support
// ============================================

// Set requires_grad
export fn tensor_set_requires_grad(t: *Tensor, requires_grad: bool): void {
    if t != null {
        t.requires_grad = requires_grad
        if requires_grad {
            t.flags = t.flags | TENSOR_REQUIRES_GRAD
        } else {
            t.flags = t.flags & ~TENSOR_REQUIRES_GRAD
        }
    }
}

// Zero gradients
export fn tensor_zero_grad(t: *Tensor): void {
    if t != null and t.grad != null {
        let numel = tensor_numel(t.grad)
        let elem_size = dtype_size(t.grad.dtype)
        basics.memset(t.grad.data, 0, numel * elem_size)
    }
}

// Accumulate gradient
export fn tensor_accumulate_grad(t: *Tensor, grad: *Tensor): void {
    if t == null or grad == null or !t.requires_grad {
        return
    }

    if t.grad == null {
        t.grad = tensor_clone(grad)
    } else {
        // Add to existing gradient
        let numel = tensor_numel(t.grad)
        if t.dtype == DTYPE_F32 {
            let g = t.grad.data as *f32
            let dg = grad.data as *f32
            var i: u64 = 0
            while i < numel {
                g[i] = g[i] + dg[i]
                i = i + 1
            }
        } else if t.dtype == DTYPE_F64 {
            let g = t.grad.data as *f64
            let dg = grad.data as *f64
            var i: u64 = 0
            while i < numel {
                g[i] = g[i] + dg[i]
                i = i + 1
            }
        }
    }
}

// ============================================
// Utility Functions
// ============================================

// Print tensor info
export fn tensor_print(t: *Tensor): void {
    if t == null {
        basics.print("Tensor: null\n")
        return
    }

    basics.print("Tensor(shape=[")
    var i: u32 = 0
    while i < t.ndim {
        basics.print_int(t.shape[i] as i32)
        if i < t.ndim - 1 {
            basics.print(", ")
        }
        i = i + 1
    }
    basics.print("], dtype=")

    if t.dtype == DTYPE_F32 {
        basics.print("f32")
    } else if t.dtype == DTYPE_F64 {
        basics.print("f64")
    } else if t.dtype == DTYPE_I32 {
        basics.print("i32")
    } else if t.dtype == DTYPE_I64 {
        basics.print("i64")
    }

    basics.print(", device=")
    if t.device == DEVICE_CPU {
        basics.print("cpu")
    } else {
        basics.print("gpu")
    }

    if t.requires_grad {
        basics.print(", requires_grad=true")
    }

    basics.print(")\n")
}

// Seed random number generator
export fn tensor_manual_seed(seed: u64): void {
    rand_state = seed
}
