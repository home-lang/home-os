// Virtual Memory Manager for home-os
// 4-level page table implementation for x86-64
// Based on Home's kernel package: ~/Code/home/packages/kernel/src/paging.zig

import cpu from "./os/cpu.home"
import serial from "./serial.home"

// Constants
let PAGE_SIZE: usize = 4096
let PAGE_SIZE_2M: usize = 2097152      // 2MB = 512 * 4KB
let PAGE_SIZE_1G: usize = 1073741824   // 1GB = 512 * 2MB
let PAGE_SHIFT: usize = 12
let PAGE_SHIFT_2M: usize = 21
let PAGE_SHIFT_1G: usize = 30
let ENTRIES_PER_TABLE: usize = 512

// Feature detection flags
var huge_pages_2m_supported: bool = false
var huge_pages_1g_supported: bool = false

// Page Table Entry Flags (64-bit)
packed struct PageFlags: u64 {
    present: bool,           // Page is present in memory
    writable: bool,          // Page is writable
    user: bool,              // User mode accessible
    write_through: bool,     // Write-through caching
    cache_disable: bool,     // Cache disabled
    accessed: bool,          // Page has been accessed
    dirty: bool,             // Page has been written to
    huge: bool,              // Huge page (2MB/1GB)
    global: bool,            // Global page (not flushed on CR3 reload)
    available1: u3,          // Available for OS use
    address: u40,            // Physical address (bits 12-51)
    available2: u11,         // Available for OS use
    no_execute: bool,        // No execute
}

// Page Table Entry
struct PageTableEntry {
    flags: PageFlags,
}

// Page Table (512 entries, 4KB aligned)
struct PageTable {
    entries: [512]PageTableEntry align(4096),
}

// Virtual Address decomposition
packed struct VirtualAddress: u64 {
    offset: u12,        // Page offset (bits 0-11)
    pt_index: u9,       // PT index (bits 12-20)
    pd_index: u9,       // PD index (bits 21-29)
    pdpt_index: u9,     // PDPT index (bits 30-38)
    pml4_index: u9,     // PML4 index (bits 39-47)
    sign_ext: u16,      // Sign extension (bits 48-63)
}

// Kernel address space starts at higher half
let KERNEL_BASE: u64 = 0xFFFFFFFF80000000

// Page tables (will be allocated properly later)
let pml4: PageTable align(4096) = undefined
let pdpt: PageTable align(4096) = undefined
let pd: PageTable align(4096) = undefined
let pt: PageTable align(4096) = undefined

// Create a page table entry
fn createPageEntry(phys_addr: u64, writable: bool, user: bool, no_exec: bool): PageFlags {
    return PageFlags {
        present: true,
        writable: writable,
        user: user,
        write_through: false,
        cache_disable: false,
        accessed: false,
        dirty: false,
        huge: false,
        global: false,
        available1: 0,
        address: @truncate(phys_addr >> 12),
        available2: 0,
        no_execute: no_exec,
    }
}

// Create a huge page entry (2MB or 1GB)
fn createHugePageEntry(phys_addr: u64, writable: bool, user: bool, no_exec: bool): PageFlags {
    return PageFlags {
        present: true,
        writable: writable,
        user: user,
        write_through: false,
        cache_disable: false,
        accessed: false,
        dirty: false,
        huge: true,  // PS bit set for huge page
        global: false,
        available1: 0,
        address: @truncate(phys_addr >> 12),
        available2: 0,
        no_execute: no_exec,
    }
}

// Get physical address from page flags
fn getPhysicalAddress(flags: PageFlags): u64 {
    return @as(u64, flags.address) << 12
}

// Initialize virtual memory manager
export fn init() {
    serial.writeString("Initializing Virtual Memory Manager...\n")

    // Zero out all page tables
    let i = 0
    loop {
        if i >= 512 {
            break
        }
        pml4.entries[i] = PageTableEntry {
            flags: PageFlags {
                present: false,
                writable: false,
                user: false,
                write_through: false,
                cache_disable: false,
                accessed: false,
                dirty: false,
                huge: false,
                global: false,
                available1: 0,
                address: 0,
                available2: 0,
                no_execute: false,
            }
        }
        pdpt.entries[i] = PageTableEntry {
            flags: PageFlags {
                present: false,
                writable: false,
                user: false,
                write_through: false,
                cache_disable: false,
                accessed: false,
                dirty: false,
                huge: false,
                global: false,
                available1: 0,
                address: 0,
                available2: 0,
                no_execute: false,
            }
        }
        pd.entries[i] = PageTableEntry {
            flags: PageFlags {
                present: false,
                writable: false,
                user: false,
                write_through: false,
                cache_disable: false,
                accessed: false,
                dirty: false,
                huge: false,
                global: false,
                available1: 0,
                address: 0,
                available2: 0,
                no_execute: false,
            }
        }
        pt.entries[i] = PageTableEntry {
            flags: PageFlags {
                present: false,
                writable: false,
                user: false,
                write_through: false,
                cache_disable: false,
                accessed: false,
                dirty: false,
                huge: false,
                global: false,
                available1: 0,
                address: 0,
                available2: 0,
                no_execute: false,
            }
        }
        i = i + 1
    }

    serial.writeString("VMM: Page tables zeroed\n")

    // Set up identity mapping for first 4MB (for kernel)
    // This maps virtual 0x0 - 0x400000 to physical 0x0 - 0x400000
    mapKernelSpace()

    serial.writeString("VMM: Kernel space mapped\n")

    // Load CR3 with PML4 address
    let pml4_addr = @intFromPtr(&pml4)
    serial.writeString("VMM: Loading CR3 with PML4 at ")
    serial.writeHex(pml4_addr)
    serial.writeString("\n")

    cpu.write_cr3(pml4_addr)

    serial.writeString("VMM initialized successfully!\n")
}

// Map kernel space (identity mapping for first 4MB)
fn mapKernelSpace() {
    // Link PML4[0] -> PDPT
    let pdpt_addr = @intFromPtr(&pdpt)
    pml4.entries[0].flags = createPageEntry(pdpt_addr, true, false, false)

    // Link PDPT[0] -> PD
    let pd_addr = @intFromPtr(&pd)
    pdpt.entries[0].flags = createPageEntry(pd_addr, true, false, false)

    // Link PD[0] -> PT
    let pt_addr = @intFromPtr(&pt)
    pd.entries[0].flags = createPageEntry(pt_addr, true, false, false)

    // Map first 2MB (512 pages * 4KB = 2MB) with identity mapping
    let i = 0
    loop {
        if i >= 512 {
            break
        }
        let phys_addr = i * PAGE_SIZE
        pt.entries[i].flags = createPageEntry(phys_addr, true, false, false)
        i = i + 1
    }
}

// Map a virtual address to a physical address
export fn mapPage(virt_addr: u64, phys_addr: u64, writable: bool, user: bool) {
    // Decompose virtual address
    let vaddr: VirtualAddress = @bitCast(virt_addr)

    // For now, we only support mapping in the first PML4 entry
    // TODO: Implement full 4-level page table walking and allocation

    serial.writeString("VMM: Mapping virtual ")
    serial.writeHex(virt_addr)
    serial.writeString(" to physical ")
    serial.writeHex(phys_addr)
    serial.writeString("\n")

    // Create page entry
    let entry = createPageEntry(phys_addr, writable, user, false)

    // Set the page table entry
    // This is simplified - in a real implementation, we'd walk the page tables
    let pt_index = vaddr.pt_index
    pt.entries[pt_index].flags = entry

    // Invalidate TLB for this address
    cpu.invlpg(virt_addr)
}

// Unmap a virtual address
export fn unmapPage(virt_addr: u64) {
    let vaddr: VirtualAddress = @bitCast(virt_addr)
    let pt_index = vaddr.pt_index

    // Clear the page table entry
    pt.entries[pt_index].flags = PageFlags {
        present: false,
        writable: false,
        user: false,
        write_through: false,
        cache_disable: false,
        accessed: false,
        dirty: false,
        huge: false,
        global: false,
        available1: 0,
        address: 0,
        available2: 0,
        no_execute: false,
    }

    // Invalidate TLB for this address
    cpu.invlpg(virt_addr)
}

// Get physical address for a virtual address
export fn getPhysicalAddr(virt_addr: u64): u64 {
    let vaddr: VirtualAddress = @bitCast(virt_addr)
    let pt_index = vaddr.pt_index

    let entry = pt.entries[pt_index].flags
    if !entry.present {
        return 0  // Page not present
    }

    let phys_base = getPhysicalAddress(entry)
    return phys_base + vaddr.offset
}

// Align address down to page boundary
export fn alignDown(addr: u64): u64 {
    return addr & ~(PAGE_SIZE - 1)
}

// Align address up to page boundary
export fn alignUp(addr: u64): u64 {
    return (addr + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1)
}

// ============================================================================
// Huge Page Support (2MB and 1GB)
// ============================================================================

// Detect huge page support via CPUID
export fn detectHugePageSupport() {
    // CPUID leaf 0x01: EDX bit 3 = PSE (Page Size Extension, 2MB pages)
    // CPUID leaf 0x80000001: EDX bit 26 = PDPE1GB (1GB pages)

    // Check PSE for 2MB pages
    let eax: u32 = undefined
    let ebx: u32 = undefined
    let ecx: u32 = undefined
    let edx: u32 = undefined

    cpu.cpuid(0x01, &eax, &ebx, &ecx, &edx)
    huge_pages_2m_supported = (edx & (1 << 3)) != 0  // PSE bit

    // Check extended features for 1GB pages
    cpu.cpuid(0x80000001, &eax, &ebx, &ecx, &edx)
    huge_pages_1g_supported = (edx & (1 << 26)) != 0  // PDPE1GB bit

    serial.writeString("VMM: 2MB huge pages: ")
    serial.writeString(if huge_pages_2m_supported { "supported\n" } else { "not supported\n" })
    serial.writeString("VMM: 1GB huge pages: ")
    serial.writeString(if huge_pages_1g_supported { "supported\n" } else { "not supported\n" })
}

// Map a 2MB huge page (virtual must be 2MB aligned, physical must be 2MB aligned)
// Uses PD entry with PS bit set, skipping the PT level
export fn mapHugePage2M(virt_addr: u64, phys_addr: u64, writable: bool, user: bool): bool {
    if !huge_pages_2m_supported {
        serial.writeString("VMM: 2MB huge pages not supported\n")
        return false
    }

    // Verify alignment
    if (virt_addr & (PAGE_SIZE_2M - 1)) != 0 {
        serial.writeString("VMM: Virtual address not 2MB aligned\n")
        return false
    }
    if (phys_addr & (PAGE_SIZE_2M - 1)) != 0 {
        serial.writeString("VMM: Physical address not 2MB aligned\n")
        return false
    }

    let vaddr: VirtualAddress = @bitCast(virt_addr)

    serial.writeString("VMM: Mapping 2MB huge page: virt=")
    serial.writeHex(virt_addr)
    serial.writeString(" -> phys=")
    serial.writeHex(phys_addr)
    serial.writeString("\n")

    // For 2MB huge page: PD entry points directly to physical page with PS bit set
    // PT level is skipped (vaddr.pt_index becomes part of page offset)
    let pd_index = vaddr.pd_index

    // Ensure PML4 and PDPT are set up
    if !pml4.entries[vaddr.pml4_index].flags.present {
        let pdpt_addr = @intFromPtr(&pdpt)
        pml4.entries[vaddr.pml4_index].flags = createPageEntry(pdpt_addr, true, user, false)
    }

    if !pdpt.entries[vaddr.pdpt_index].flags.present {
        let pd_addr = @intFromPtr(&pd)
        pdpt.entries[vaddr.pdpt_index].flags = createPageEntry(pd_addr, true, user, false)
    }

    // Set PD entry as huge page (PS bit = 1)
    pd.entries[pd_index].flags = createHugePageEntry(phys_addr, writable, user, false)

    // Invalidate TLB for entire 2MB range
    cpu.invlpg(virt_addr)

    return true
}

// Map a 1GB huge page (virtual must be 1GB aligned, physical must be 1GB aligned)
// Uses PDPT entry with PS bit set, skipping PD and PT levels
export fn mapHugePage1G(virt_addr: u64, phys_addr: u64, writable: bool, user: bool): bool {
    if !huge_pages_1g_supported {
        serial.writeString("VMM: 1GB huge pages not supported\n")
        return false
    }

    // Verify alignment
    if (virt_addr & (PAGE_SIZE_1G - 1)) != 0 {
        serial.writeString("VMM: Virtual address not 1GB aligned\n")
        return false
    }
    if (phys_addr & (PAGE_SIZE_1G - 1)) != 0 {
        serial.writeString("VMM: Physical address not 1GB aligned\n")
        return false
    }

    let vaddr: VirtualAddress = @bitCast(virt_addr)

    serial.writeString("VMM: Mapping 1GB huge page: virt=")
    serial.writeHex(virt_addr)
    serial.writeString(" -> phys=")
    serial.writeHex(phys_addr)
    serial.writeString("\n")

    // For 1GB huge page: PDPT entry points directly to physical page with PS bit set
    // PD and PT levels are skipped
    let pdpt_index = vaddr.pdpt_index

    // Ensure PML4 is set up
    if !pml4.entries[vaddr.pml4_index].flags.present {
        let pdpt_addr = @intFromPtr(&pdpt)
        pml4.entries[vaddr.pml4_index].flags = createPageEntry(pdpt_addr, true, user, false)
    }

    // Set PDPT entry as 1GB huge page (PS bit = 1)
    pdpt.entries[pdpt_index].flags = createHugePageEntry(phys_addr, writable, user, false)

    // Invalidate TLB
    cpu.invlpg(virt_addr)

    return true
}

// Unmap a 2MB huge page
export fn unmapHugePage2M(virt_addr: u64) {
    let vaddr: VirtualAddress = @bitCast(virt_addr)
    let pd_index = vaddr.pd_index

    pd.entries[pd_index].flags = PageFlags {
        present: false,
        writable: false,
        user: false,
        write_through: false,
        cache_disable: false,
        accessed: false,
        dirty: false,
        huge: false,
        global: false,
        available1: 0,
        address: 0,
        available2: 0,
        no_execute: false,
    }

    cpu.invlpg(virt_addr)
}

// Unmap a 1GB huge page
export fn unmapHugePage1G(virt_addr: u64) {
    let vaddr: VirtualAddress = @bitCast(virt_addr)
    let pdpt_index = vaddr.pdpt_index

    pdpt.entries[pdpt_index].flags = PageFlags {
        present: false,
        writable: false,
        user: false,
        write_through: false,
        cache_disable: false,
        accessed: false,
        dirty: false,
        huge: false,
        global: false,
        available1: 0,
        address: 0,
        available2: 0,
        no_execute: false,
    }

    cpu.invlpg(virt_addr)
}

// Get page size info for an address
export fn getPageSize(virt_addr: u64): usize {
    let vaddr: VirtualAddress = @bitCast(virt_addr)

    // Check if PDPT entry is a 1GB huge page
    if pdpt.entries[vaddr.pdpt_index].flags.present {
        if pdpt.entries[vaddr.pdpt_index].flags.huge {
            return PAGE_SIZE_1G
        }
    }

    // Check if PD entry is a 2MB huge page
    if pd.entries[vaddr.pd_index].flags.present {
        if pd.entries[vaddr.pd_index].flags.huge {
            return PAGE_SIZE_2M
        }
    }

    // Default 4KB page
    return PAGE_SIZE
}

// Align address to 2MB boundary
export fn alignDown2M(addr: u64): u64 {
    return addr & ~@as(u64, PAGE_SIZE_2M - 1)
}

export fn alignUp2M(addr: u64): u64 {
    return (addr + PAGE_SIZE_2M - 1) & ~@as(u64, PAGE_SIZE_2M - 1)
}

// Align address to 1GB boundary
export fn alignDown1G(addr: u64): u64 {
    return addr & ~@as(u64, PAGE_SIZE_1G - 1)
}

export fn alignUp1G(addr: u64): u64 {
    return (addr + PAGE_SIZE_1G - 1) & ~@as(u64, PAGE_SIZE_1G - 1)
}

// Check if huge pages are available
export fn supports2MPages(): bool {
    return huge_pages_2m_supported
}

export fn supports1GPages(): bool {
    return huge_pages_1g_supported
}
