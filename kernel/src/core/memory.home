// home-os Kernel - Memory Management
// Real implementations only - no placeholders!

import "foundation.home" as foundation

// ============================================================================
// CONSTANTS
// ============================================================================

const PAGE_SIZE: u64 = 4096
const MAX_MEMORY: u64 = 1024 * 1024 * 1024  // 1GB
const BITMAP_SIZE: u32 = (MAX_MEMORY / PAGE_SIZE / 8)

// ============================================================================
// PHYSICAL MEMORY MANAGER (Real implementation)
// ============================================================================

var pmm_bitmap: [32768]u8  // 1GB / 4096 / 8 = 32768 bytes
var pmm_total_pages: u32 = 0
var pmm_used_pages: u32 = 0
var pmm_initialized: u32 = 0

export fn pmm_init(memory_size: u64) {
  if pmm_initialized == 1 { return }
  
  pmm_total_pages = (memory_size / PAGE_SIZE)
  pmm_used_pages = 0
  
  // Clear bitmap
  var i: u32 = 0
  while i < BITMAP_SIZE {
    pmm_bitmap[i] = 0
    i = i + 1
  }
  
  pmm_initialized = 1
  foundation.serial_write_string("[PMM] Initialized with ")
  foundation.serial_write_string(" pages\n")
}

fn pmm_set_bit(page: u32) {
  var byte_index: u32 = page / 8
  var bit_index: u32 = page % 8
  pmm_bitmap[byte_index] = pmm_bitmap[byte_index] | (1 << bit_index)
}

fn pmm_clear_bit(page: u32) {
  var byte_index: u32 = page / 8
  var bit_index: u32 = page % 8
  pmm_bitmap[byte_index] = pmm_bitmap[byte_index] & ~(1 << bit_index)
}

fn pmm_test_bit(page: u32): u32 {
  var byte_index: u32 = page / 8
  var bit_index: u32 = page % 8
  return (pmm_bitmap[byte_index] & (1 << bit_index)) != 0
}

export fn pmm_alloc_page(): u64 {
  var page: u32 = 0
  
  while page < pmm_total_pages {
    if pmm_test_bit(page) == 0 {
      pmm_set_bit(page)
      pmm_used_pages = pmm_used_pages + 1
      return page * PAGE_SIZE
    }
    page = page + 1
  }
  
  return 0  // Out of memory
}

export fn pmm_free_page(addr: u64) {
  var page: u32 = addr / PAGE_SIZE
  
  if pmm_test_bit(page) == 1 {
    pmm_clear_bit(page)
    pmm_used_pages = pmm_used_pages - 1
  }
}

export fn pmm_get_used_pages(): u32 {
  return pmm_used_pages
}

export fn pmm_get_free_pages(): u32 {
  return pmm_total_pages - pmm_used_pages
}

// ============================================================================
// VIRTUAL MEMORY MANAGER (Real implementation)
// ============================================================================

const PML4_ENTRIES: u32 = 512
const PDPT_ENTRIES: u32 = 512
const PD_ENTRIES: u32 = 512
const PT_ENTRIES: u32 = 512

const PAGE_PRESENT: u64 = 1 << 0
const PAGE_WRITE: u64 = 1 << 1
const PAGE_USER: u64 = 1 << 2

var vmm_pml4: u64 = 0
var vmm_initialized: u32 = 0

export fn vmm_init() {
  if vmm_initialized == 1 { return }
  
  // Allocate PML4
  vmm_pml4 = pmm_alloc_page()
  if vmm_pml4 == 0 { return }
  
  // Clear PML4
  var i: u32 = 0
  while i < PML4_ENTRIES {
    var entry_addr: u64 = vmm_pml4 + (i * 8)
    @ptrToInt(entry_addr, u64) = 0
    i = i + 1
  }
  
  // Identity map first 4MB for kernel
  vmm_map_range(0, 0, 4 * 1024 * 1024, PAGE_PRESENT | PAGE_WRITE)
  
  // Load CR3
  asm volatile ("mov %[pml4], %%cr3"
    :
    : [pml4] "r" (vmm_pml4)
  )
  
  vmm_initialized = 1
  foundation.serial_write_string("[VMM] Initialized\n")
}

export fn vmm_map_page(virt: u64, phys: u64, flags: u64): u32 {
  var pml4_index: u32 = (virt >> 39) & 0x1FF
  var pdpt_index: u32 = (virt >> 30) & 0x1FF
  var pd_index: u32 = (virt >> 21) & 0x1FF
  var pt_index: u32 = (virt >> 12) & 0x1FF
  
  // Get or create PDPT
  var pml4_entry_addr: u64 = vmm_pml4 + (pml4_index * 8)
  var pdpt: u64 = @intToPtr(pml4_entry_addr, u64) & ~0xFFF
  
  if pdpt == 0 {
    pdpt = pmm_alloc_page()
    if pdpt == 0 { return 1 }
    @ptrToInt(pml4_entry_addr, u64) = pdpt | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }
  
  // Get or create PD
  var pdpt_entry_addr: u64 = pdpt + (pdpt_index * 8)
  var pd: u64 = @intToPtr(pdpt_entry_addr, u64) & ~0xFFF
  
  if pd == 0 {
    pd = pmm_alloc_page()
    if pd == 0 { return 1 }
    @ptrToInt(pdpt_entry_addr, u64) = pd | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }
  
  // Get or create PT
  var pd_entry_addr: u64 = pd + (pd_index * 8)
  var pt: u64 = @intToPtr(pd_entry_addr, u64) & ~0xFFF
  
  if pt == 0 {
    pt = pmm_alloc_page()
    if pt == 0 { return 1 }
    @ptrToInt(pd_entry_addr, u64) = pt | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }
  
  // Map page
  var pt_entry_addr: u64 = pt + (pt_index * 8)
  @ptrToInt(pt_entry_addr, u64) = phys | flags
  
  // Invalidate TLB
  asm volatile ("invlpg (%[virt])"
    :
    : [virt] "r" (virt)
  )
  
  return 0
}

export fn vmm_unmap_page(virt: u64) {
  var pml4_index: u32 = (virt >> 39) & 0x1FF
  var pdpt_index: u32 = (virt >> 30) & 0x1FF
  var pd_index: u32 = (virt >> 21) & 0x1FF
  var pt_index: u32 = (virt >> 12) & 0x1FF
  
  var pml4_entry_addr: u64 = vmm_pml4 + (pml4_index * 8)
  var pdpt: u64 = @intToPtr(pml4_entry_addr, u64) & ~0xFFF
  if pdpt == 0 { return }
  
  var pdpt_entry_addr: u64 = pdpt + (pdpt_index * 8)
  var pd: u64 = @intToPtr(pdpt_entry_addr, u64) & ~0xFFF
  if pd == 0 { return }
  
  var pd_entry_addr: u64 = pd + (pd_index * 8)
  var pt: u64 = @intToPtr(pd_entry_addr, u64) & ~0xFFF
  if pt == 0 { return }
  
  var pt_entry_addr: u64 = pt + (pt_index * 8)
  @ptrToInt(pt_entry_addr, u64) = 0
  
  // Invalidate TLB
  asm volatile ("invlpg (%[virt])"
    :
    : [virt] "r" (virt)
  )
}

fn vmm_map_range(virt_start: u64, phys_start: u64, size: u64, flags: u64) {
  var virt: u64 = virt_start
  var phys: u64 = phys_start
  var end: u64 = virt_start + size
  
  while virt < end {
    vmm_map_page(virt, phys, flags)
    virt = virt + PAGE_SIZE
    phys = phys + PAGE_SIZE
  }
}

// ============================================================================
// HEAP ALLOCATOR (Real implementation)
// ============================================================================

const HEAP_START: u64 = 0x0000_1000_0000_0000
const HEAP_SIZE: u64 = 16 * 1024 * 1024  // 16MB

struct HeapBlock {
  size: u64,
  used: u32,
  next: u64
}

var heap_head: u64 = 0
var heap_initialized: u32 = 0

export fn heap_init() {
  if heap_initialized == 1 { return }
  
  // Allocate physical pages for heap
  var pages_needed: u32 = HEAP_SIZE / PAGE_SIZE
  var i: u32 = 0
  
  while i < pages_needed {
    var phys: u64 = pmm_alloc_page()
    if phys == 0 { return }
    vmm_map_page(HEAP_START + (i * PAGE_SIZE), phys, PAGE_PRESENT | PAGE_WRITE)
    i = i + 1
  }
  
  // Initialize first block
  heap_head = HEAP_START
  var block: u64 = heap_head
  @ptrToInt(block, u64) = HEAP_SIZE - @sizeOf(HeapBlock)  // size
  @ptrToInt(block + 8, u32) = 0  // used
  @ptrToInt(block + 12, u64) = 0  // next
  
  heap_initialized = 1
  foundation.serial_write_string("[Heap] Initialized\n")
}

export fn kmalloc(size: u64): u64 {
  if size == 0 { return 0 }
  
  // Align to 8 bytes
  var aligned_size: u64 = (size + 7) & ~7
  
  var current: u64 = heap_head
  
  while current != 0 {
    var block_size: u64 = @intToPtr(current, u64)
    var block_used: u32 = @intToPtr(current + 8, u32)
    var block_next: u64 = @intToPtr(current + 12, u64)
    
    if block_used == 0 and block_size >= aligned_size {
      // Found suitable block
      @ptrToInt(current + 8, u32) = 1  // Mark as used
      
      // Split block if there's enough space
      if block_size >= aligned_size + @sizeOf(HeapBlock) + 64 {
        var new_block: u64 = current + @sizeOf(HeapBlock) + aligned_size
        @ptrToInt(new_block, u64) = block_size - aligned_size - @sizeOf(HeapBlock)
        @ptrToInt(new_block + 8, u32) = 0
        @ptrToInt(new_block + 12, u64) = block_next
        
        @ptrToInt(current, u64) = aligned_size
        @ptrToInt(current + 12, u64) = new_block
      }
      
      return current + @sizeOf(HeapBlock)
    }
    
    current = block_next
  }
  
  return 0  // Out of memory
}

export fn kfree(ptr: u64) {
  if ptr == 0 { return }

  var block: u64 = ptr - @sizeOf(HeapBlock)
  @ptrToInt(block + 8, u32) = 0  // Mark as free

  // Coalesce adjacent free blocks
  coalesce_free_blocks()
}

// Coalesce adjacent free blocks to reduce fragmentation
fn coalesce_free_blocks() {
  var current: u64 = heap_head

  while current != 0 {
    var current_size: u64 = @intToPtr(current, u64)
    var current_used: u32 = @intToPtr(current + 8, u32)
    var current_next: u64 = @intToPtr(current + 12, u64)

    // If current block is free, try to merge with next block
    if current_used == 0 and current_next != 0 {
      var next_used: u32 = @intToPtr(current_next + 8, u32)

      // If next block is also free, merge them
      if next_used == 0 {
        var next_size: u64 = @intToPtr(current_next, u64)
        var next_next: u64 = @intToPtr(current_next + 12, u64)

        // Merge: current absorbs next
        var new_size: u64 = current_size + @sizeOf(HeapBlock) + next_size
        @ptrToInt(current, u64) = new_size
        @ptrToInt(current + 12, u64) = next_next

        // Don't advance - check if we can merge more
        continue
      }
    }

    current = current_next
  }
}

// Get heap statistics
export fn heap_get_stats(total: *u64, used: *u64, free: *u64, largest_free: *u64) {
  var total_size: u64 = 0
  var used_size: u64 = 0
  var free_size: u64 = 0
  var max_free: u64 = 0

  var current: u64 = heap_head

  while current != 0 {
    var block_size: u64 = @intToPtr(current, u64)
    var block_used: u32 = @intToPtr(current + 8, u32)
    var block_next: u64 = @intToPtr(current + 12, u64)

    total_size = total_size + block_size + @sizeOf(HeapBlock)

    if block_used == 1 {
      used_size = used_size + block_size
    } else {
      free_size = free_size + block_size
      if block_size > max_free {
        max_free = block_size
      }
    }

    current = block_next
  }

  @ptrToInt(total, u64) = total_size
  @ptrToInt(used, u64) = used_size
  @ptrToInt(free, u64) = free_size
  @ptrToInt(largest_free, u64) = max_free
}

// Compact heap by moving used blocks together
export fn heap_compact() {
  // First, coalesce all free blocks
  coalesce_free_blocks()

  // Note: Full compaction would require updating all pointers
  // which is complex without garbage collection. This basic
  // implementation just merges free blocks.

  foundation.serial_write_string("[Heap] Compacted\n")
}

// Check heap integrity
export fn heap_check(): u32 {
  var current: u64 = heap_head
  var prev: u64 = 0
  var block_count: u32 = 0

  while current != 0 {
    var block_size: u64 = @intToPtr(current, u64)
    var block_used: u32 = @intToPtr(current + 8, u32)
    var block_next: u64 = @intToPtr(current + 12, u64)

    // Validate block
    if block_size == 0 {
      foundation.serial_write_string("[Heap] ERROR: Zero-size block at ")
      foundation.serial_write_hex(current)
      foundation.serial_write_string("\n")
      return 1
    }

    if block_used != 0 and block_used != 1 {
      foundation.serial_write_string("[Heap] ERROR: Invalid used flag at ")
      foundation.serial_write_hex(current)
      foundation.serial_write_string("\n")
      return 2
    }

    // Check for cycles
    if block_next != 0 and block_next <= current {
      foundation.serial_write_string("[Heap] ERROR: Backward/cycle link at ")
      foundation.serial_write_hex(current)
      foundation.serial_write_string("\n")
      return 3
    }

    // Check block boundaries
    var expected_next: u64 = current + @sizeOf(HeapBlock) + block_size
    if block_next != 0 and block_next != expected_next {
      // Blocks don't have to be contiguous, but check for overlap
      if block_next < expected_next {
        foundation.serial_write_string("[Heap] ERROR: Overlapping blocks at ")
        foundation.serial_write_hex(current)
        foundation.serial_write_string("\n")
        return 4
      }
    }

    prev = current
    current = block_next
    block_count = block_count + 1

    // Prevent infinite loop
    if block_count > 1000000 {
      foundation.serial_write_string("[Heap] ERROR: Too many blocks (possible cycle)\n")
      return 5
    }
  }

  return 0  // Heap OK
}

// Reallocate memory (resize allocation)
export fn krealloc(ptr: u64, new_size: u64): u64 {
  if ptr == 0 {
    return kmalloc(new_size)
  }

  if new_size == 0 {
    kfree(ptr)
    return 0
  }

  var block: u64 = ptr - @sizeOf(HeapBlock)
  var old_size: u64 = @intToPtr(block, u64)

  // If new size fits in current block, just return
  var aligned_new: u64 = (new_size + 7) & ~7
  if aligned_new <= old_size {
    return ptr
  }

  // Check if we can expand into next block
  var next: u64 = @intToPtr(block + 12, u64)
  if next != 0 {
    var next_used: u32 = @intToPtr(next + 8, u32)
    if next_used == 0 {
      var next_size: u64 = @intToPtr(next, u64)
      var combined: u64 = old_size + @sizeOf(HeapBlock) + next_size

      if combined >= aligned_new {
        // Merge with next block
        var next_next: u64 = @intToPtr(next + 12, u64)
        @ptrToInt(block, u64) = combined
        @ptrToInt(block + 12, u64) = next_next

        // Split if there's enough space left
        if combined >= aligned_new + @sizeOf(HeapBlock) + 64 {
          var new_block: u64 = block + @sizeOf(HeapBlock) + aligned_new
          @ptrToInt(new_block, u64) = combined - aligned_new - @sizeOf(HeapBlock)
          @ptrToInt(new_block + 8, u32) = 0
          @ptrToInt(new_block + 12, u64) = next_next

          @ptrToInt(block, u64) = aligned_new
          @ptrToInt(block + 12, u64) = new_block
        }

        return ptr
      }
    }
  }

  // Need to allocate new block and copy
  var new_ptr: u64 = kmalloc(new_size)
  if new_ptr == 0 {
    return 0
  }

  // Copy old data
  var copy_size: u64 = old_size
  if new_size < copy_size {
    copy_size = new_size
  }

  var i: u64 = 0
  while i < copy_size {
    @ptrToInt(new_ptr + i, u8) = @intToPtr(ptr + i, u8)
    i = i + 1
  }

  // Free old block
  kfree(ptr)

  return new_ptr
}

// Allocate zeroed memory
export fn kcalloc(count: u64, size: u64): u64 {
  var total: u64 = count * size
  var ptr: u64 = kmalloc(total)

  if ptr != 0 {
    var i: u64 = 0
    while i < total {
      @ptrToInt(ptr + i, u8) = 0
      i = i + 1
    }
  }

  return ptr
}

// ============================================================================
// INITIALIZATION
// ============================================================================

export fn memory_init(memory_size: u64) {
  pmm_init(memory_size)
  vmm_init()
  heap_init()
  
  foundation.serial_write_string("[Memory] Initialized\n")
}
