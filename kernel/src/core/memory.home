// home-os Kernel - Memory Management
// Real implementations only - no placeholders!

import "foundation.home" as foundation

// ============================================================================
// CONSTANTS
// ============================================================================

const PAGE_SIZE: u64 = 4096
const MAX_MEMORY: u64 = 1024 * 1024 * 1024  // 1GB
const BITMAP_SIZE: u32 = (MAX_MEMORY / PAGE_SIZE / 8)

// ============================================================================
// PHYSICAL MEMORY MANAGER (Real implementation)
// ============================================================================

var pmm_bitmap: [32768]u8  // 1GB / 4096 / 8 = 32768 bytes
var pmm_total_pages: u32 = 0
var pmm_used_pages: u32 = 0
var pmm_initialized: u32 = 0

export fn pmm_init(memory_size: u64) {
  if pmm_initialized == 1 { return }
  
  pmm_total_pages = (memory_size / PAGE_SIZE)
  pmm_used_pages = 0
  
  // Clear bitmap
  var i: u32 = 0
  while i < BITMAP_SIZE {
    pmm_bitmap[i] = 0
    i = i + 1
  }
  
  pmm_initialized = 1
  foundation.serial_write_string("[PMM] Initialized with ")
  foundation.serial_write_string(" pages\n")
}

fn pmm_set_bit(page: u32) {
  var byte_index: u32 = page / 8
  var bit_index: u32 = page % 8
  pmm_bitmap[byte_index] = pmm_bitmap[byte_index] | (1 << bit_index)
}

fn pmm_clear_bit(page: u32) {
  var byte_index: u32 = page / 8
  var bit_index: u32 = page % 8
  pmm_bitmap[byte_index] = pmm_bitmap[byte_index] & ~(1 << bit_index)
}

fn pmm_test_bit(page: u32): u32 {
  var byte_index: u32 = page / 8
  var bit_index: u32 = page % 8
  return (pmm_bitmap[byte_index] & (1 << bit_index)) != 0
}

export fn pmm_alloc_page(): u64 {
  var page: u32 = 0
  
  while page < pmm_total_pages {
    if pmm_test_bit(page) == 0 {
      pmm_set_bit(page)
      pmm_used_pages = pmm_used_pages + 1
      return page * PAGE_SIZE
    }
    page = page + 1
  }
  
  return 0  // Out of memory
}

export fn pmm_free_page(addr: u64) {
  var page: u32 = addr / PAGE_SIZE
  
  if pmm_test_bit(page) == 1 {
    pmm_clear_bit(page)
    pmm_used_pages = pmm_used_pages - 1
  }
}

export fn pmm_get_used_pages(): u32 {
  return pmm_used_pages
}

export fn pmm_get_free_pages(): u32 {
  return pmm_total_pages - pmm_used_pages
}

// ============================================================================
// VIRTUAL MEMORY MANAGER (Real implementation)
// ============================================================================

const PML4_ENTRIES: u32 = 512
const PDPT_ENTRIES: u32 = 512
const PD_ENTRIES: u32 = 512
const PT_ENTRIES: u32 = 512

const PAGE_PRESENT: u64 = 1 << 0
const PAGE_WRITE: u64 = 1 << 1
const PAGE_USER: u64 = 1 << 2

var vmm_pml4: u64 = 0
var vmm_initialized: u32 = 0

export fn vmm_init() {
  if vmm_initialized == 1 { return }
  
  // Allocate PML4
  vmm_pml4 = pmm_alloc_page()
  if vmm_pml4 == 0 { return }
  
  // Clear PML4
  var i: u32 = 0
  while i < PML4_ENTRIES {
    var entry_addr: u64 = vmm_pml4 + (i * 8)
    @ptrToInt(entry_addr, u64) = 0
    i = i + 1
  }
  
  // Identity map first 4MB for kernel
  vmm_map_range(0, 0, 4 * 1024 * 1024, PAGE_PRESENT | PAGE_WRITE)
  
  // Load CR3
  asm volatile ("mov %[pml4], %%cr3"
    :
    : [pml4] "r" (vmm_pml4)
  )
  
  vmm_initialized = 1
  foundation.serial_write_string("[VMM] Initialized\n")
}

export fn vmm_map_page(virt: u64, phys: u64, flags: u64): u32 {
  var pml4_index: u32 = (virt >> 39) & 0x1FF
  var pdpt_index: u32 = (virt >> 30) & 0x1FF
  var pd_index: u32 = (virt >> 21) & 0x1FF
  var pt_index: u32 = (virt >> 12) & 0x1FF
  
  // Get or create PDPT
  var pml4_entry_addr: u64 = vmm_pml4 + (pml4_index * 8)
  var pdpt: u64 = @intToPtr(pml4_entry_addr, u64) & ~0xFFF
  
  if pdpt == 0 {
    pdpt = pmm_alloc_page()
    if pdpt == 0 { return 1 }
    @ptrToInt(pml4_entry_addr, u64) = pdpt | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }
  
  // Get or create PD
  var pdpt_entry_addr: u64 = pdpt + (pdpt_index * 8)
  var pd: u64 = @intToPtr(pdpt_entry_addr, u64) & ~0xFFF
  
  if pd == 0 {
    pd = pmm_alloc_page()
    if pd == 0 { return 1 }
    @ptrToInt(pdpt_entry_addr, u64) = pd | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }
  
  // Get or create PT
  var pd_entry_addr: u64 = pd + (pd_index * 8)
  var pt: u64 = @intToPtr(pd_entry_addr, u64) & ~0xFFF
  
  if pt == 0 {
    pt = pmm_alloc_page()
    if pt == 0 { return 1 }
    @ptrToInt(pd_entry_addr, u64) = pt | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }
  
  // Map page
  var pt_entry_addr: u64 = pt + (pt_index * 8)
  @ptrToInt(pt_entry_addr, u64) = phys | flags
  
  // Invalidate TLB
  asm volatile ("invlpg (%[virt])"
    :
    : [virt] "r" (virt)
  )
  
  return 0
}

export fn vmm_unmap_page(virt: u64) {
  var pml4_index: u32 = (virt >> 39) & 0x1FF
  var pdpt_index: u32 = (virt >> 30) & 0x1FF
  var pd_index: u32 = (virt >> 21) & 0x1FF
  var pt_index: u32 = (virt >> 12) & 0x1FF
  
  var pml4_entry_addr: u64 = vmm_pml4 + (pml4_index * 8)
  var pdpt: u64 = @intToPtr(pml4_entry_addr, u64) & ~0xFFF
  if pdpt == 0 { return }
  
  var pdpt_entry_addr: u64 = pdpt + (pdpt_index * 8)
  var pd: u64 = @intToPtr(pdpt_entry_addr, u64) & ~0xFFF
  if pd == 0 { return }
  
  var pd_entry_addr: u64 = pd + (pd_index * 8)
  var pt: u64 = @intToPtr(pd_entry_addr, u64) & ~0xFFF
  if pt == 0 { return }
  
  var pt_entry_addr: u64 = pt + (pt_index * 8)
  @ptrToInt(pt_entry_addr, u64) = 0
  
  // Invalidate TLB
  asm volatile ("invlpg (%[virt])"
    :
    : [virt] "r" (virt)
  )
}

fn vmm_map_range(virt_start: u64, phys_start: u64, size: u64, flags: u64) {
  var virt: u64 = virt_start
  var phys: u64 = phys_start
  var end: u64 = virt_start + size
  
  while virt < end {
    vmm_map_page(virt, phys, flags)
    virt = virt + PAGE_SIZE
    phys = phys + PAGE_SIZE
  }
}

// ============================================================================
// HEAP ALLOCATOR (Real implementation)
// ============================================================================

const HEAP_START: u64 = 0x0000_1000_0000_0000
const HEAP_SIZE: u64 = 16 * 1024 * 1024  // 16MB

struct HeapBlock {
  size: u64,
  used: u32,
  next: u64
}

var heap_head: u64 = 0
var heap_initialized: u32 = 0

export fn heap_init() {
  if heap_initialized == 1 { return }
  
  // Allocate physical pages for heap
  var pages_needed: u32 = HEAP_SIZE / PAGE_SIZE
  var i: u32 = 0
  
  while i < pages_needed {
    var phys: u64 = pmm_alloc_page()
    if phys == 0 { return }
    vmm_map_page(HEAP_START + (i * PAGE_SIZE), phys, PAGE_PRESENT | PAGE_WRITE)
    i = i + 1
  }
  
  // Initialize first block
  heap_head = HEAP_START
  var block: u64 = heap_head
  @ptrToInt(block, u64) = HEAP_SIZE - @sizeOf(HeapBlock)  // size
  @ptrToInt(block + 8, u32) = 0  // used
  @ptrToInt(block + 12, u64) = 0  // next
  
  heap_initialized = 1
  foundation.serial_write_string("[Heap] Initialized\n")
}

export fn kmalloc(size: u64): u64 {
  if size == 0 { return 0 }
  
  // Align to 8 bytes
  var aligned_size: u64 = (size + 7) & ~7
  
  var current: u64 = heap_head
  
  while current != 0 {
    var block_size: u64 = @intToPtr(current, u64)
    var block_used: u32 = @intToPtr(current + 8, u32)
    var block_next: u64 = @intToPtr(current + 12, u64)
    
    if block_used == 0 and block_size >= aligned_size {
      // Found suitable block
      @ptrToInt(current + 8, u32) = 1  // Mark as used
      
      // Split block if there's enough space
      if block_size >= aligned_size + @sizeOf(HeapBlock) + 64 {
        var new_block: u64 = current + @sizeOf(HeapBlock) + aligned_size
        @ptrToInt(new_block, u64) = block_size - aligned_size - @sizeOf(HeapBlock)
        @ptrToInt(new_block + 8, u32) = 0
        @ptrToInt(new_block + 12, u64) = block_next
        
        @ptrToInt(current, u64) = aligned_size
        @ptrToInt(current + 12, u64) = new_block
      }
      
      return current + @sizeOf(HeapBlock)
    }
    
    current = block_next
  }
  
  return 0  // Out of memory
}

export fn kfree(ptr: u64) {
  if ptr == 0 { return }

  var block: u64 = ptr - @sizeOf(HeapBlock)
  @ptrToInt(block + 8, u32) = 0  // Mark as free

  // Coalesce adjacent free blocks
  coalesce_free_blocks()
}

// Coalesce adjacent free blocks to reduce fragmentation
fn coalesce_free_blocks() {
  var current: u64 = heap_head

  while current != 0 {
    var current_size: u64 = @intToPtr(current, u64)
    var current_used: u32 = @intToPtr(current + 8, u32)
    var current_next: u64 = @intToPtr(current + 12, u64)

    // If current block is free, try to merge with next block
    if current_used == 0 and current_next != 0 {
      var next_used: u32 = @intToPtr(current_next + 8, u32)

      // If next block is also free, merge them
      if next_used == 0 {
        var next_size: u64 = @intToPtr(current_next, u64)
        var next_next: u64 = @intToPtr(current_next + 12, u64)

        // Merge: current absorbs next
        var new_size: u64 = current_size + @sizeOf(HeapBlock) + next_size
        @ptrToInt(current, u64) = new_size
        @ptrToInt(current + 12, u64) = next_next

        // Don't advance - check if we can merge more
        continue
      }
    }

    current = current_next
  }
}

// Get heap statistics
export fn heap_get_stats(total: *u64, used: *u64, free: *u64, largest_free: *u64) {
  var total_size: u64 = 0
  var used_size: u64 = 0
  var free_size: u64 = 0
  var max_free: u64 = 0

  var current: u64 = heap_head

  while current != 0 {
    var block_size: u64 = @intToPtr(current, u64)
    var block_used: u32 = @intToPtr(current + 8, u32)
    var block_next: u64 = @intToPtr(current + 12, u64)

    total_size = total_size + block_size + @sizeOf(HeapBlock)

    if block_used == 1 {
      used_size = used_size + block_size
    } else {
      free_size = free_size + block_size
      if block_size > max_free {
        max_free = block_size
      }
    }

    current = block_next
  }

  @ptrToInt(total, u64) = total_size
  @ptrToInt(used, u64) = used_size
  @ptrToInt(free, u64) = free_size
  @ptrToInt(largest_free, u64) = max_free
}

// Compact heap by moving used blocks together
export fn heap_compact() {
  // First, coalesce all free blocks
  coalesce_free_blocks()

  // Note: Full compaction would require updating all pointers
  // which is complex without garbage collection. This basic
  // implementation just merges free blocks.

  foundation.serial_write_string("[Heap] Compacted\n")
}

// Check heap integrity
export fn heap_check(): u32 {
  var current: u64 = heap_head
  var prev: u64 = 0
  var block_count: u32 = 0

  while current != 0 {
    var block_size: u64 = @intToPtr(current, u64)
    var block_used: u32 = @intToPtr(current + 8, u32)
    var block_next: u64 = @intToPtr(current + 12, u64)

    // Validate block
    if block_size == 0 {
      foundation.serial_write_string("[Heap] ERROR: Zero-size block at ")
      foundation.serial_write_hex(current)
      foundation.serial_write_string("\n")
      return 1
    }

    if block_used != 0 and block_used != 1 {
      foundation.serial_write_string("[Heap] ERROR: Invalid used flag at ")
      foundation.serial_write_hex(current)
      foundation.serial_write_string("\n")
      return 2
    }

    // Check for cycles
    if block_next != 0 and block_next <= current {
      foundation.serial_write_string("[Heap] ERROR: Backward/cycle link at ")
      foundation.serial_write_hex(current)
      foundation.serial_write_string("\n")
      return 3
    }

    // Check block boundaries
    var expected_next: u64 = current + @sizeOf(HeapBlock) + block_size
    if block_next != 0 and block_next != expected_next {
      // Blocks don't have to be contiguous, but check for overlap
      if block_next < expected_next {
        foundation.serial_write_string("[Heap] ERROR: Overlapping blocks at ")
        foundation.serial_write_hex(current)
        foundation.serial_write_string("\n")
        return 4
      }
    }

    prev = current
    current = block_next
    block_count = block_count + 1

    // Prevent infinite loop
    if block_count > 1000000 {
      foundation.serial_write_string("[Heap] ERROR: Too many blocks (possible cycle)\n")
      return 5
    }
  }

  return 0  // Heap OK
}

// Reallocate memory (resize allocation)
export fn krealloc(ptr: u64, new_size: u64): u64 {
  if ptr == 0 {
    return kmalloc(new_size)
  }

  if new_size == 0 {
    kfree(ptr)
    return 0
  }

  var block: u64 = ptr - @sizeOf(HeapBlock)
  var old_size: u64 = @intToPtr(block, u64)

  // If new size fits in current block, just return
  var aligned_new: u64 = (new_size + 7) & ~7
  if aligned_new <= old_size {
    return ptr
  }

  // Check if we can expand into next block
  var next: u64 = @intToPtr(block + 12, u64)
  if next != 0 {
    var next_used: u32 = @intToPtr(next + 8, u32)
    if next_used == 0 {
      var next_size: u64 = @intToPtr(next, u64)
      var combined: u64 = old_size + @sizeOf(HeapBlock) + next_size

      if combined >= aligned_new {
        // Merge with next block
        var next_next: u64 = @intToPtr(next + 12, u64)
        @ptrToInt(block, u64) = combined
        @ptrToInt(block + 12, u64) = next_next

        // Split if there's enough space left
        if combined >= aligned_new + @sizeOf(HeapBlock) + 64 {
          var new_block: u64 = block + @sizeOf(HeapBlock) + aligned_new
          @ptrToInt(new_block, u64) = combined - aligned_new - @sizeOf(HeapBlock)
          @ptrToInt(new_block + 8, u32) = 0
          @ptrToInt(new_block + 12, u64) = next_next

          @ptrToInt(block, u64) = aligned_new
          @ptrToInt(block + 12, u64) = new_block
        }

        return ptr
      }
    }
  }

  // Need to allocate new block and copy
  var new_ptr: u64 = kmalloc(new_size)
  if new_ptr == 0 {
    return 0
  }

  // Copy old data
  var copy_size: u64 = old_size
  if new_size < copy_size {
    copy_size = new_size
  }

  var i: u64 = 0
  while i < copy_size {
    @ptrToInt(new_ptr + i, u8) = @intToPtr(ptr + i, u8)
    i = i + 1
  }

  // Free old block
  kfree(ptr)

  return new_ptr
}

// Allocate zeroed memory
export fn kcalloc(count: u64, size: u64): u64 {
  var total: u64 = count * size
  var ptr: u64 = kmalloc(total)

  if ptr != 0 {
    var i: u64 = 0
    while i < total {
      @ptrToInt(ptr + i, u8) = 0
      i = i + 1
    }
  }

  return ptr
}

// ============================================================================
// MEMORY UTILITIES
// ============================================================================

export fn memset(addr: u64, value: u8, size: u64) {
  var ptr: *u8 = @ptrFromInt(addr)
  var i: u64 = 0
  while i < size {
    ptr[i] = value
    i = i + 1
  }
}

export fn memcpy(dst: u64, src: u64, size: u64) {
  var d: *u8 = @ptrFromInt(dst)
  var s: *u8 = @ptrFromInt(src)
  var i: u64 = 0
  while i < size {
    d[i] = s[i]
    i = i + 1
  }
}

// ============================================================================
// PER-PROCESS ADDRESS SPACE MANAGEMENT
// ============================================================================

const MAX_PROCESSES: u32 = 256
const PAGE_COW: u64 = 1 << 9  // Copy-on-write flag

// Per-process PML4 table pointers
var process_pml4: [u64; 256]
var process_pml4_initialized: u32 = 0

fn init_process_pml4() {
  if process_pml4_initialized == 1 { return }

  var i: u32 = 0
  while i < MAX_PROCESSES {
    process_pml4[i] = 0
    i = i + 1
  }

  // Process 0 uses kernel PML4
  process_pml4[0] = vmm_pml4

  process_pml4_initialized = 1
}

// Create address space for a new process
export fn vmm_create_address_space(pid: u32): u64 {
  init_process_pml4()

  if pid >= MAX_PROCESSES { return 0 }

  // Allocate PML4 for this process
  var pml4: u64 = pmm_alloc_page()
  if pml4 == 0 { return 0 }

  // Clear PML4
  memset(pml4, 0, PAGE_SIZE)

  // Copy kernel mappings (upper half)
  var kernel_pml4: u64 = vmm_pml4
  var i: u32 = 256
  while i < 512 {
    var src_entry: u64 = @intToPtr(kernel_pml4 + i * 8, u64)
    if src_entry != 0 {
      @ptrToInt(pml4 + i * 8, u64) = src_entry
    }
    i = i + 1
  }

  process_pml4[pid] = pml4
  return pml4
}

// Get PML4 for a process
fn get_process_pml4(pid: u32): u64 {
  init_process_pml4()

  if pid >= MAX_PROCESSES { return vmm_pml4 }
  if process_pml4[pid] == 0 { return vmm_pml4 }
  return process_pml4[pid]
}

// Map page in a process's address space
export fn vmm_map_page_process(pid: u32, virt: u64, phys: u64, prot: u32): u32 {
  var pml4: u64 = get_process_pml4(pid)

  // Convert protection flags
  var flags: u64 = PAGE_PRESENT
  if (prot & 2) != 0 {  // PROT_WRITE
    flags = flags | PAGE_WRITE
  }
  if (prot & 4) == 0 {  // Not PROT_EXEC (NX bit)
    // NX bit would be set here if supported
  }
  flags = flags | PAGE_USER

  var pml4_index: u32 = @truncate((virt >> 39) & 0x1FF, u32)
  var pdpt_index: u32 = @truncate((virt >> 30) & 0x1FF, u32)
  var pd_index: u32 = @truncate((virt >> 21) & 0x1FF, u32)
  var pt_index: u32 = @truncate((virt >> 12) & 0x1FF, u32)

  // Get or create PDPT
  var pml4_entry_addr: u64 = pml4 + (pml4_index * 8)
  var pdpt: u64 = @intToPtr(pml4_entry_addr, u64) & ~0xFFF

  if pdpt == 0 {
    pdpt = pmm_alloc_page()
    if pdpt == 0 { return 1 }
    memset(pdpt, 0, PAGE_SIZE)
    @ptrToInt(pml4_entry_addr, u64) = pdpt | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }

  // Get or create PD
  var pdpt_entry_addr: u64 = pdpt + (pdpt_index * 8)
  var pd: u64 = @intToPtr(pdpt_entry_addr, u64) & ~0xFFF

  if pd == 0 {
    pd = pmm_alloc_page()
    if pd == 0 { return 1 }
    memset(pd, 0, PAGE_SIZE)
    @ptrToInt(pdpt_entry_addr, u64) = pd | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }

  // Get or create PT
  var pd_entry_addr: u64 = pd + (pd_index * 8)
  var pt: u64 = @intToPtr(pd_entry_addr, u64) & ~0xFFF

  if pt == 0 {
    pt = pmm_alloc_page()
    if pt == 0 { return 1 }
    memset(pt, 0, PAGE_SIZE)
    @ptrToInt(pd_entry_addr, u64) = pt | PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  }

  // Map page
  var pt_entry_addr: u64 = pt + (pt_index * 8)
  @ptrToInt(pt_entry_addr, u64) = phys | flags

  // Flush TLB if this is the current address space
  if pid == 0 or pml4 == vmm_pml4 {
    asm volatile ("invlpg (%[virt])"
      :
      : [virt] "r" (virt)
    )
  }

  return 0
}

// Unmap page in a process's address space
export fn vmm_unmap_page_process(pid: u32, virt: u64) {
  var pml4: u64 = get_process_pml4(pid)

  var pml4_index: u32 = @truncate((virt >> 39) & 0x1FF, u32)
  var pdpt_index: u32 = @truncate((virt >> 30) & 0x1FF, u32)
  var pd_index: u32 = @truncate((virt >> 21) & 0x1FF, u32)
  var pt_index: u32 = @truncate((virt >> 12) & 0x1FF, u32)

  var pml4_entry_addr: u64 = pml4 + (pml4_index * 8)
  var pdpt: u64 = @intToPtr(pml4_entry_addr, u64) & ~0xFFF
  if pdpt == 0 { return }

  var pdpt_entry_addr: u64 = pdpt + (pdpt_index * 8)
  var pd: u64 = @intToPtr(pdpt_entry_addr, u64) & ~0xFFF
  if pd == 0 { return }

  var pd_entry_addr: u64 = pd + (pd_index * 8)
  var pt: u64 = @intToPtr(pd_entry_addr, u64) & ~0xFFF
  if pt == 0 { return }

  var pt_entry_addr: u64 = pt + (pt_index * 8)
  @ptrToInt(pt_entry_addr, u64) = 0

  // Flush TLB if this is the current address space
  if pid == 0 or pml4 == vmm_pml4 {
    asm volatile ("invlpg (%[virt])"
      :
      : [virt] "r" (virt)
    )
  }
}

// Get physical address for a virtual address in a process
export fn vmm_get_physical(pid: u32, virt: u64): u64 {
  var pml4: u64 = get_process_pml4(pid)

  var pml4_index: u32 = @truncate((virt >> 39) & 0x1FF, u32)
  var pdpt_index: u32 = @truncate((virt >> 30) & 0x1FF, u32)
  var pd_index: u32 = @truncate((virt >> 21) & 0x1FF, u32)
  var pt_index: u32 = @truncate((virt >> 12) & 0x1FF, u32)

  var pml4_entry_addr: u64 = pml4 + (pml4_index * 8)
  var pml4_entry: u64 = @intToPtr(pml4_entry_addr, u64)
  if (pml4_entry & PAGE_PRESENT) == 0 { return 0 }

  var pdpt: u64 = pml4_entry & ~0xFFF
  var pdpt_entry: u64 = @intToPtr(pdpt + pdpt_index * 8, u64)
  if (pdpt_entry & PAGE_PRESENT) == 0 { return 0 }

  var pd: u64 = pdpt_entry & ~0xFFF
  var pd_entry: u64 = @intToPtr(pd + pd_index * 8, u64)
  if (pd_entry & PAGE_PRESENT) == 0 { return 0 }

  var pt: u64 = pd_entry & ~0xFFF
  var pt_entry: u64 = @intToPtr(pt + pt_index * 8, u64)
  if (pt_entry & PAGE_PRESENT) == 0 { return 0 }

  return (pt_entry & ~0xFFF) + (virt & 0xFFF)
}

// Set page protection
export fn vmm_set_protection(pid: u32, virt: u64, prot: u32) {
  var pml4: u64 = get_process_pml4(pid)

  var pml4_index: u32 = @truncate((virt >> 39) & 0x1FF, u32)
  var pdpt_index: u32 = @truncate((virt >> 30) & 0x1FF, u32)
  var pd_index: u32 = @truncate((virt >> 21) & 0x1FF, u32)
  var pt_index: u32 = @truncate((virt >> 12) & 0x1FF, u32)

  var pml4_entry: u64 = @intToPtr(pml4 + pml4_index * 8, u64)
  if (pml4_entry & PAGE_PRESENT) == 0 { return }

  var pdpt: u64 = pml4_entry & ~0xFFF
  var pdpt_entry: u64 = @intToPtr(pdpt + pdpt_index * 8, u64)
  if (pdpt_entry & PAGE_PRESENT) == 0 { return }

  var pd: u64 = pdpt_entry & ~0xFFF
  var pd_entry: u64 = @intToPtr(pd + pd_index * 8, u64)
  if (pd_entry & PAGE_PRESENT) == 0 { return }

  var pt: u64 = pd_entry & ~0xFFF
  var pt_entry_addr: u64 = pt + pt_index * 8
  var pt_entry: u64 = @intToPtr(pt_entry_addr, u64)
  if (pt_entry & PAGE_PRESENT) == 0 { return }

  // Get physical address
  var phys: u64 = pt_entry & ~0xFFF

  // Build new flags
  var flags: u64 = PAGE_PRESENT | PAGE_USER
  if (prot & 2) != 0 {  // PROT_WRITE
    flags = flags | PAGE_WRITE
  }

  // Update entry
  @ptrToInt(pt_entry_addr, u64) = phys | flags

  // Flush TLB
  if pid == 0 or pml4 == vmm_pml4 {
    asm volatile ("invlpg (%[virt])"
      :
      : [virt] "r" (virt)
    )
  }
}

// Set copy-on-write for a page
export fn vmm_set_cow(pid: u32, virt: u64) {
  var pml4: u64 = get_process_pml4(pid)

  var pml4_index: u32 = @truncate((virt >> 39) & 0x1FF, u32)
  var pdpt_index: u32 = @truncate((virt >> 30) & 0x1FF, u32)
  var pd_index: u32 = @truncate((virt >> 21) & 0x1FF, u32)
  var pt_index: u32 = @truncate((virt >> 12) & 0x1FF, u32)

  var pml4_entry: u64 = @intToPtr(pml4 + pml4_index * 8, u64)
  if (pml4_entry & PAGE_PRESENT) == 0 { return }

  var pdpt: u64 = pml4_entry & ~0xFFF
  var pdpt_entry: u64 = @intToPtr(pdpt + pdpt_index * 8, u64)
  if (pdpt_entry & PAGE_PRESENT) == 0 { return }

  var pd: u64 = pdpt_entry & ~0xFFF
  var pd_entry: u64 = @intToPtr(pd + pd_index * 8, u64)
  if (pd_entry & PAGE_PRESENT) == 0 { return }

  var pt: u64 = pd_entry & ~0xFFF
  var pt_entry_addr: u64 = pt + pt_index * 8
  var pt_entry: u64 = @intToPtr(pt_entry_addr, u64)
  if (pt_entry & PAGE_PRESENT) == 0 { return }

  // Set COW flag and remove write permission
  pt_entry = pt_entry | PAGE_COW
  pt_entry = pt_entry & ~PAGE_WRITE

  @ptrToInt(pt_entry_addr, u64) = pt_entry

  // Flush TLB
  if pid == 0 or pml4 == vmm_pml4 {
    asm volatile ("invlpg (%[virt])"
      :
      : [virt] "r" (virt)
    )
  }
}

// Handle COW page fault
export fn vmm_handle_cow(pid: u32, virt: u64): u32 {
  var pml4: u64 = get_process_pml4(pid)

  var pml4_index: u32 = @truncate((virt >> 39) & 0x1FF, u32)
  var pdpt_index: u32 = @truncate((virt >> 30) & 0x1FF, u32)
  var pd_index: u32 = @truncate((virt >> 21) & 0x1FF, u32)
  var pt_index: u32 = @truncate((virt >> 12) & 0x1FF, u32)

  var pml4_entry: u64 = @intToPtr(pml4 + pml4_index * 8, u64)
  if (pml4_entry & PAGE_PRESENT) == 0 { return 1 }

  var pdpt: u64 = pml4_entry & ~0xFFF
  var pdpt_entry: u64 = @intToPtr(pdpt + pdpt_index * 8, u64)
  if (pdpt_entry & PAGE_PRESENT) == 0 { return 1 }

  var pd: u64 = pdpt_entry & ~0xFFF
  var pd_entry: u64 = @intToPtr(pd + pd_index * 8, u64)
  if (pd_entry & PAGE_PRESENT) == 0 { return 1 }

  var pt: u64 = pd_entry & ~0xFFF
  var pt_entry_addr: u64 = pt + pt_index * 8
  var pt_entry: u64 = @intToPtr(pt_entry_addr, u64)
  if (pt_entry & PAGE_PRESENT) == 0 { return 1 }

  // Check if this is a COW page
  if (pt_entry & PAGE_COW) == 0 {
    return 1  // Not a COW fault
  }

  var old_phys: u64 = pt_entry & ~0xFFF

  // Allocate new page
  var new_phys: u64 = pmm_alloc_page()
  if new_phys == 0 { return 1 }

  // Copy contents
  memcpy(new_phys, old_phys, PAGE_SIZE)

  // Map with write permission, clear COW flag
  var flags: u64 = PAGE_PRESENT | PAGE_WRITE | PAGE_USER
  @ptrToInt(pt_entry_addr, u64) = new_phys | flags

  // Flush TLB
  asm volatile ("invlpg (%[virt])"
    :
    : [virt] "r" (virt)
  )

  return 0
}

// Switch to a process's address space
export fn vmm_switch_address_space(pid: u32) {
  var pml4: u64 = get_process_pml4(pid)
  if pml4 == 0 { return }

  asm volatile ("mov %[pml4], %%cr3"
    :
    : [pml4] "r" (pml4)
  )
}

// Free a process's address space
export fn vmm_free_address_space(pid: u32) {
  if pid >= MAX_PROCESSES { return }
  if pid == 0 { return }  // Never free kernel address space

  var pml4: u64 = process_pml4[pid]
  if pml4 == 0 { return }

  // Free user-space page tables (entries 0-255)
  var pml4_idx: u32 = 0
  while pml4_idx < 256 {
    var pml4_entry: u64 = @intToPtr(pml4 + pml4_idx * 8, u64)
    if (pml4_entry & PAGE_PRESENT) != 0 {
      var pdpt: u64 = pml4_entry & ~0xFFF

      var pdpt_idx: u32 = 0
      while pdpt_idx < 512 {
        var pdpt_entry: u64 = @intToPtr(pdpt + pdpt_idx * 8, u64)
        if (pdpt_entry & PAGE_PRESENT) != 0 {
          var pd: u64 = pdpt_entry & ~0xFFF

          var pd_idx: u32 = 0
          while pd_idx < 512 {
            var pd_entry: u64 = @intToPtr(pd + pd_idx * 8, u64)
            if (pd_entry & PAGE_PRESENT) != 0 {
              var pt: u64 = pd_entry & ~0xFFF

              // Free page table entries
              var pt_idx: u32 = 0
              while pt_idx < 512 {
                var pt_entry: u64 = @intToPtr(pt + pt_idx * 8, u64)
                if (pt_entry & PAGE_PRESENT) != 0 {
                  var phys: u64 = pt_entry & ~0xFFF
                  pmm_free_page(phys)
                }
                pt_idx = pt_idx + 1
              }

              pmm_free_page(pt)
            }
            pd_idx = pd_idx + 1
          }

          pmm_free_page(pd)
        }
        pdpt_idx = pdpt_idx + 1
      }

      pmm_free_page(pdpt)
    }
    pml4_idx = pml4_idx + 1
  }

  pmm_free_page(pml4)
  process_pml4[pid] = 0
}

// ============================================================================
// INITIALIZATION
// ============================================================================

export fn memory_init(memory_size: u64) {
  pmm_init(memory_size)
  vmm_init()
  heap_init()
  init_process_pml4()

  foundation.serial_write_string("[Memory] Initialized\n")
}
