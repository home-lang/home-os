// home-os Kernel - Unicode Support
// UTF-8 encoding/decoding and text processing

import "../core/foundation.home" as foundation

export fn utf8_decode(input: u64, codepoint_out: u64) -> u32 {
  var byte1: u8 = @intToPtr(input, u8)
  
  if (byte1 & 0x80) == 0 {
    // 1-byte sequence (ASCII)
    @intToPtr(codepoint_out, u32) = byte1
    return 1
  } else if (byte1 & 0xE0) == 0xC0 {
    // 2-byte sequence
    var byte2: u8 = @intToPtr(input + 1, u8)
    var codepoint: u32 = ((byte1 & 0x1F) << 6) | (byte2 & 0x3F)
    @intToPtr(codepoint_out, u32) = codepoint
    return 2
  } else if (byte1 & 0xF0) == 0xE0 {
    // 3-byte sequence
    var byte2: u8 = @intToPtr(input + 1, u8)
    var byte3: u8 = @intToPtr(input + 2, u8)
    var codepoint: u32 = ((byte1 & 0x0F) << 12) | ((byte2 & 0x3F) << 6) | (byte3 & 0x3F)
    @intToPtr(codepoint_out, u32) = codepoint
    return 3
  } else if (byte1 & 0xF8) == 0xF0 {
    // 4-byte sequence
    var byte2: u8 = @intToPtr(input + 1, u8)
    var byte3: u8 = @intToPtr(input + 2, u8)
    var byte4: u8 = @intToPtr(input + 3, u8)
    var codepoint: u32 = ((byte1 & 0x07) << 18) | ((byte2 & 0x3F) << 12) | ((byte3 & 0x3F) << 6) | (byte4 & 0x3F)
    @intToPtr(codepoint_out, u32) = codepoint
    return 4
  }
  
  return 0  // Invalid
}

export fn utf8_encode(codepoint: u32, output: u64) -> u32 {
  if codepoint < 0x80 {
    // 1-byte sequence
    @intToPtr(output, u8) = @truncate(codepoint, u8)
    return 1
  } else if codepoint < 0x800 {
    // 2-byte sequence
    @intToPtr(output, u8) = @truncate(0xC0 | (codepoint >> 6), u8)
    @intToPtr(output + 1, u8) = @truncate(0x80 | (codepoint & 0x3F), u8)
    return 2
  } else if codepoint < 0x10000 {
    // 3-byte sequence
    @intToPtr(output, u8) = @truncate(0xE0 | (codepoint >> 12), u8)
    @intToPtr(output + 1, u8) = @truncate(0x80 | ((codepoint >> 6) & 0x3F), u8)
    @intToPtr(output + 2, u8) = @truncate(0x80 | (codepoint & 0x3F), u8)
    return 3
  } else if codepoint < 0x110000 {
    // 4-byte sequence
    @intToPtr(output, u8) = @truncate(0xF0 | (codepoint >> 18), u8)
    @intToPtr(output + 1, u8) = @truncate(0x80 | ((codepoint >> 12) & 0x3F), u8)
    @intToPtr(output + 2, u8) = @truncate(0x80 | ((codepoint >> 6) & 0x3F), u8)
    @intToPtr(output + 3, u8) = @truncate(0x80 | (codepoint & 0x3F), u8)
    return 4
  }
  
  return 0  // Invalid
}

export fn utf8_strlen(str: u64) -> u32 {
  var len: u32 = 0
  var pos: u32 = 0
  
  while @intToPtr(str + pos, u8) != 0 {
    var codepoint: u32 = 0
    var bytes: u32 = utf8_decode(str + pos, @ptrFromInt(codepoint))
    
    if bytes == 0 { break }
    
    pos = pos + bytes
    len = len + 1
  }
  
  return len
}

export fn utf8_validate(str: u64, max_len: u32) -> u32 {
  var pos: u32 = 0
  
  while pos < max_len {
    var byte: u8 = @intToPtr(str + pos, u8)
    if byte == 0 { return 1 }
    
    var codepoint: u32 = 0
    var bytes: u32 = utf8_decode(str + pos, @ptrFromInt(codepoint))
    
    if bytes == 0 { return 0 }  // Invalid
    
    pos = pos + bytes
  }
  
  return 1
}

export fn utf8_to_upper(input: u64, output: u64, max_len: u32) -> u32 {
  var in_pos: u32 = 0
  var out_pos: u32 = 0
  
  while in_pos < max_len {
    var byte: u8 = @intToPtr(input + in_pos, u8)
    if byte == 0 { break }
    
    var codepoint: u32 = 0
    var bytes: u32 = utf8_decode(input + in_pos, @ptrFromInt(codepoint))
    
    // Simple ASCII uppercase
    if codepoint >= 'a' and codepoint <= 'z' {
      codepoint = codepoint - 32
    }
    
    var out_bytes: u32 = utf8_encode(codepoint, output + out_pos)
    
    in_pos = in_pos + bytes
    out_pos = out_pos + out_bytes
  }
  
  @intToPtr(output + out_pos, u8) = 0
  return out_pos
}
