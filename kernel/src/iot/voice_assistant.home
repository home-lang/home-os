// home-os Voice Assistant - Local, Privacy-Focused
// On-device wake word detection, speech recognition, NLU, and TTS

import "../core/foundation.home" as foundation
import "home_automation.home" as home_auto

// ============================================================================
// Voice Assistant Configuration
// ============================================================================

// Assistant states
const VA_STATE_IDLE: u8 = 0              // Listening for wake word
const VA_STATE_LISTENING: u8 = 1         // Recording user command
const VA_STATE_PROCESSING: u8 = 2        // Processing speech
const VA_STATE_RESPONDING: u8 = 3        // Speaking response
const VA_STATE_ERROR: u8 = 4             // Error state
const VA_STATE_DISABLED: u8 = 5          // Disabled

// Wake word options
const WAKE_WORD_HOMEOS: u8 = 0           // "Home OS"
const WAKE_WORD_COMPUTER: u8 = 1         // "Computer"
const WAKE_WORD_ASSISTANT: u8 = 2        // "Assistant"
const WAKE_WORD_CUSTOM: u8 = 3           // User-defined

// Intent categories
const INTENT_UNKNOWN: u8 = 0
const INTENT_LIGHT_CONTROL: u8 = 1       // Lights on/off/dim
const INTENT_THERMOSTAT: u8 = 2          // Temperature control
const INTENT_LOCK_CONTROL: u8 = 3        // Lock/unlock doors
const INTENT_MEDIA_CONTROL: u8 = 4       // Play/pause/volume
const INTENT_TIMER: u8 = 5               // Set timer/alarm
const INTENT_WEATHER: u8 = 6             // Weather query
const INTENT_TIME: u8 = 7                // Time/date query
const INTENT_REMINDER: u8 = 8            // Set reminder
const INTENT_SCENE: u8 = 9               // Activate scene
const INTENT_DEVICE_STATUS: u8 = 10      // Query device state
const INTENT_HELP: u8 = 11               // Help request
const INTENT_CANCEL: u8 = 12             // Cancel operation
const INTENT_CONFIRM: u8 = 13            // Confirm action
const INTENT_ROUTINE: u8 = 14            // Run routine

// Action types
const ACTION_ON: u8 = 0
const ACTION_OFF: u8 = 1
const ACTION_TOGGLE: u8 = 2
const ACTION_SET: u8 = 3
const ACTION_INCREASE: u8 = 4
const ACTION_DECREASE: u8 = 5
const ACTION_QUERY: u8 = 6

// Audio settings
const SAMPLE_RATE: u32 = 16000           // 16 kHz for speech
const CHANNELS: u8 = 1                   // Mono
const BITS_PER_SAMPLE: u8 = 16           // 16-bit PCM
const FRAME_SIZE_MS: u32 = 30            // 30ms frames
const LISTEN_TIMEOUT_MS: u32 = 5000      // 5 second listen timeout
const SILENCE_THRESHOLD: u16 = 500       // Silence detection threshold

// Buffer sizes
const AUDIO_BUFFER_SIZE: u32 = 32000     // 2 seconds at 16kHz
const TRANSCRIPT_MAX_LEN: u32 = 512
const RESPONSE_MAX_LEN: u32 = 1024

const MAX_ENTITIES: u32 = 16
const MAX_WAKE_WORDS: u32 = 4
const MAX_COMMANDS: u32 = 64
const MAX_SYNONYMS: u32 = 128

// ============================================================================
// Data Structures
// ============================================================================

// Detected entity in speech
struct Entity {
    entity_type: u8,                     // room, device, number, time, etc.
    value: [u8; 64],                     // Entity value string
    start_pos: u16,                      // Position in transcript
    end_pos: u16,
    confidence: u8                       // 0-100
}

// Entity types
const ENTITY_ROOM: u8 = 0
const ENTITY_DEVICE: u8 = 1
const ENTITY_NUMBER: u8 = 2
const ENTITY_TIME: u8 = 3
const ENTITY_DURATION: u8 = 4
const ENTITY_COLOR: u8 = 5
const ENTITY_TEMPERATURE: u8 = 6
const ENTITY_PERCENTAGE: u8 = 7
const ENTITY_SCENE: u8 = 8

// Parsed intent result
struct ParsedIntent {
    intent: u8,
    action: u8,
    confidence: u8,                      // 0-100
    entities: [Entity; 16],
    entity_count: u8,
    raw_transcript: [u8; 512],
    transcript_len: u16
}

// Wake word model
struct WakeWordModel {
    word_id: u8,
    keyword: [u8; 32],
    threshold: u16,                      // Detection threshold
    enabled: u8,
    detections: u32,
    false_positives: u32
}

// Command pattern for intent matching
struct CommandPattern {
    intent: u8,
    action: u8,
    patterns: [u8; 256],                 // Pipe-separated patterns
    pattern_count: u8,
    priority: u8,
    active: u8
}

// Synonym mapping
struct Synonym {
    word: [u8; 32],
    canonical: [u8; 32],                 // Canonical form
    category: u8                         // device, room, action, etc.
}

// TTS voice configuration
struct VoiceConfig {
    voice_id: u8,
    pitch: u8,                           // 0-100
    speed: u8,                           // 0-100 (50 = normal)
    volume: u8,                          // 0-100
    language: [u8; 8]                    // "en-US", "es-ES", etc.
}

// Voice assistant configuration
struct VoiceAssistantConfig {
    enabled: u8,
    wake_word: u8,
    custom_wake_word: [u8; 32],
    listen_timeout_ms: u32,
    confirmation_required: u8,           // Require confirm for actions
    voice: VoiceConfig,
    privacy_mode: u8,                    // Don't store recordings
    local_only: u8,                      // No cloud processing
    feedback_sounds: u8                  // Play sounds on wake/done
}

// Audio buffer for recording
struct AudioBuffer {
    samples: [i16; 32000],               // 2 seconds of audio
    write_pos: u32,
    read_pos: u32,
    sample_count: u32,
    is_recording: u8,
    vad_active: u8                       // Voice activity detected
}

// Speech recognition result
struct SpeechResult {
    transcript: [u8; 512],
    transcript_len: u16,
    confidence: u8,
    duration_ms: u32,
    is_final: u8
}

// Voice assistant state
struct VoiceAssistantState {
    state: u8,
    config: VoiceAssistantConfig,
    audio_buffer: AudioBuffer,
    current_intent: ParsedIntent,
    last_response: [u8; 1024],
    response_len: u16,

    // Timing
    wake_time: u64,
    listen_start: u64,
    process_start: u64,

    // Statistics
    wake_detections: u64,
    commands_processed: u64,
    commands_successful: u64,
    commands_failed: u64,
    avg_response_time_ms: u32
}

// ============================================================================
// Global State
// ============================================================================

var va_state: VoiceAssistantState
var wake_words: [WakeWordModel; 4]
var command_patterns: [CommandPattern; 64]
var pattern_count: u32 = 0
var synonyms: [Synonym; 128]
var synonym_count: u32 = 0
var va_initialized: u32 = 0

// ============================================================================
// Initialization
// ============================================================================

export fn voice_assistant_init(): u32 {
    if va_initialized == 1 {
        return 1
    }

    // Initialize state
    va_state.state = VA_STATE_IDLE
    va_state.audio_buffer.write_pos = 0
    va_state.audio_buffer.read_pos = 0
    va_state.audio_buffer.sample_count = 0
    va_state.audio_buffer.is_recording = 0
    va_state.audio_buffer.vad_active = 0
    va_state.wake_detections = 0
    va_state.commands_processed = 0
    va_state.commands_successful = 0
    va_state.commands_failed = 0
    va_state.avg_response_time_ms = 0

    // Default configuration
    va_state.config.enabled = 1
    va_state.config.wake_word = WAKE_WORD_HOMEOS
    va_state.config.listen_timeout_ms = LISTEN_TIMEOUT_MS
    va_state.config.confirmation_required = 1
    va_state.config.privacy_mode = 1
    va_state.config.local_only = 1
    va_state.config.feedback_sounds = 1

    // Default voice settings
    va_state.config.voice.voice_id = 0
    va_state.config.voice.pitch = 50
    va_state.config.voice.speed = 50
    va_state.config.voice.volume = 80

    // Initialize wake words
    init_wake_words()

    // Initialize command patterns
    init_command_patterns()

    // Initialize synonyms
    init_synonyms()

    va_initialized = 1

    foundation.serial_write_string("[Voice Assistant] Initialized (local, privacy-focused)\n")
    return 1
}

// Initialize wake word models
fn init_wake_words() {
    // "Home OS"
    wake_words[0].word_id = WAKE_WORD_HOMEOS
    copy_string(&wake_words[0].keyword, "home os")
    wake_words[0].threshold = 800
    wake_words[0].enabled = 1
    wake_words[0].detections = 0
    wake_words[0].false_positives = 0

    // "Computer"
    wake_words[1].word_id = WAKE_WORD_COMPUTER
    copy_string(&wake_words[1].keyword, "computer")
    wake_words[1].threshold = 800
    wake_words[1].enabled = 0
    wake_words[1].detections = 0

    // "Assistant"
    wake_words[2].word_id = WAKE_WORD_ASSISTANT
    copy_string(&wake_words[2].keyword, "assistant")
    wake_words[2].threshold = 800
    wake_words[2].enabled = 0
    wake_words[2].detections = 0
}

// Initialize command patterns for intent matching
fn init_command_patterns() {
    pattern_count = 0

    // Light control patterns
    add_pattern(INTENT_LIGHT_CONTROL, ACTION_ON, "turn on|switch on|lights on|enable")
    add_pattern(INTENT_LIGHT_CONTROL, ACTION_OFF, "turn off|switch off|lights off|disable")
    add_pattern(INTENT_LIGHT_CONTROL, ACTION_SET, "dim|set brightness|brightness to")
    add_pattern(INTENT_LIGHT_CONTROL, ACTION_TOGGLE, "toggle|flip")

    // Thermostat patterns
    add_pattern(INTENT_THERMOSTAT, ACTION_SET, "set temperature|set thermostat|make it")
    add_pattern(INTENT_THERMOSTAT, ACTION_INCREASE, "warmer|increase temperature|turn up heat")
    add_pattern(INTENT_THERMOSTAT, ACTION_DECREASE, "cooler|decrease temperature|turn down heat")
    add_pattern(INTENT_THERMOSTAT, ACTION_QUERY, "what is the temperature|how warm|how cold")

    // Lock control patterns
    add_pattern(INTENT_LOCK_CONTROL, ACTION_ON, "lock|secure")
    add_pattern(INTENT_LOCK_CONTROL, ACTION_OFF, "unlock|unsecure")
    add_pattern(INTENT_LOCK_CONTROL, ACTION_QUERY, "is .* locked|lock status")

    // Media control patterns
    add_pattern(INTENT_MEDIA_CONTROL, ACTION_ON, "play|start|resume")
    add_pattern(INTENT_MEDIA_CONTROL, ACTION_OFF, "pause|stop")
    add_pattern(INTENT_MEDIA_CONTROL, ACTION_INCREASE, "volume up|louder")
    add_pattern(INTENT_MEDIA_CONTROL, ACTION_DECREASE, "volume down|quieter")
    add_pattern(INTENT_MEDIA_CONTROL, ACTION_SET, "set volume|volume to")

    // Timer patterns
    add_pattern(INTENT_TIMER, ACTION_SET, "set timer|timer for|remind me in")
    add_pattern(INTENT_TIMER, ACTION_OFF, "cancel timer|stop timer")
    add_pattern(INTENT_TIMER, ACTION_QUERY, "how much time|timer status")

    // Scene patterns
    add_pattern(INTENT_SCENE, ACTION_ON, "activate scene|run scene|scene")
    add_pattern(INTENT_ROUTINE, ACTION_ON, "run routine|start routine|routine")

    // Query patterns
    add_pattern(INTENT_TIME, ACTION_QUERY, "what time|current time|what is the time")
    add_pattern(INTENT_WEATHER, ACTION_QUERY, "weather|forecast|temperature outside")
    add_pattern(INTENT_DEVICE_STATUS, ACTION_QUERY, "status of|is .* on|is .* off")

    // Meta patterns
    add_pattern(INTENT_HELP, ACTION_QUERY, "help|what can you do|commands")
    add_pattern(INTENT_CANCEL, ACTION_OFF, "cancel|never mind|stop")
    add_pattern(INTENT_CONFIRM, ACTION_ON, "yes|confirm|do it|okay")
}

// Add a command pattern
fn add_pattern(intent: u8, action: u8, patterns: u64) {
    if pattern_count >= MAX_COMMANDS { return }

    command_patterns[pattern_count].intent = intent
    command_patterns[pattern_count].action = action
    copy_string(&command_patterns[pattern_count].patterns, patterns)
    command_patterns[pattern_count].priority = 5
    command_patterns[pattern_count].active = 1

    pattern_count = pattern_count + 1
}

// Initialize synonyms for entity normalization
fn init_synonyms() {
    synonym_count = 0

    // Room synonyms
    add_synonym("living room", "living_room", ENTITY_ROOM)
    add_synonym("lounge", "living_room", ENTITY_ROOM)
    add_synonym("front room", "living_room", ENTITY_ROOM)
    add_synonym("bedroom", "bedroom", ENTITY_ROOM)
    add_synonym("master bedroom", "master_bedroom", ENTITY_ROOM)
    add_synonym("kitchen", "kitchen", ENTITY_ROOM)
    add_synonym("bathroom", "bathroom", ENTITY_ROOM)
    add_synonym("garage", "garage", ENTITY_ROOM)
    add_synonym("office", "office", ENTITY_ROOM)
    add_synonym("study", "office", ENTITY_ROOM)

    // Device synonyms
    add_synonym("light", "light", ENTITY_DEVICE)
    add_synonym("lights", "light", ENTITY_DEVICE)
    add_synonym("lamp", "light", ENTITY_DEVICE)
    add_synonym("fan", "fan", ENTITY_DEVICE)
    add_synonym("thermostat", "thermostat", ENTITY_DEVICE)
    add_synonym("heating", "thermostat", ENTITY_DEVICE)
    add_synonym("ac", "thermostat", ENTITY_DEVICE)
    add_synonym("air conditioning", "thermostat", ENTITY_DEVICE)
    add_synonym("tv", "tv", ENTITY_DEVICE)
    add_synonym("television", "tv", ENTITY_DEVICE)
    add_synonym("door", "lock", ENTITY_DEVICE)
    add_synonym("lock", "lock", ENTITY_DEVICE)

    // Color synonyms
    add_synonym("red", "ff0000", ENTITY_COLOR)
    add_synonym("green", "00ff00", ENTITY_COLOR)
    add_synonym("blue", "0000ff", ENTITY_COLOR)
    add_synonym("white", "ffffff", ENTITY_COLOR)
    add_synonym("warm white", "ffd700", ENTITY_COLOR)
    add_synonym("cool white", "f0f8ff", ENTITY_COLOR)
    add_synonym("yellow", "ffff00", ENTITY_COLOR)
    add_synonym("purple", "800080", ENTITY_COLOR)
    add_synonym("orange", "ffa500", ENTITY_COLOR)
    add_synonym("pink", "ffc0cb", ENTITY_COLOR)
}

// Add a synonym
fn add_synonym(word: u64, canonical: u64, category: u8) {
    if synonym_count >= MAX_SYNONYMS { return }

    copy_string(&synonyms[synonym_count].word, word)
    copy_string(&synonyms[synonym_count].canonical, canonical)
    synonyms[synonym_count].category = category

    synonym_count = synonym_count + 1
}

// ============================================================================
// Wake Word Detection
// ============================================================================

// Process audio frame for wake word detection
export fn va_process_audio_frame(samples: u64, sample_count: u32): u8 {
    if va_state.config.enabled == 0 { return 0 }
    if va_state.state != VA_STATE_IDLE { return 0 }

    // Store samples in buffer for wake word detection
    store_audio_samples(samples, sample_count)

    // Run wake word detection on buffer
    var detected: u8 = detect_wake_word()

    if detected == 1 {
        va_state.wake_detections = va_state.wake_detections + 1
        va_state.wake_time = foundation.timer_get_ticks()
        va_state.state = VA_STATE_LISTENING

        // Play feedback sound if enabled
        if va_state.config.feedback_sounds == 1 {
            play_wake_sound()
        }

        // Start recording user command
        start_recording()

        foundation.serial_write_string("[Voice Assistant] Wake word detected\n")
        return 1
    }

    return 0
}

// Detect wake word using simple pattern matching
// In production, would use neural network or keyword spotting model
fn detect_wake_word(): u8 {
    // Simple energy-based voice activity detection first
    var energy: u32 = calculate_audio_energy()
    if energy < SILENCE_THRESHOLD { return 0 }

    // Would run wake word neural network here
    // For now, simulate detection based on audio characteristics
    // In real implementation: use Porcupine, Snowboy, or custom model

    var i: u32 = 0
    while i < MAX_WAKE_WORDS {
        if wake_words[i].enabled == 1 {
            // Check wake word confidence
            var confidence: u16 = run_wake_word_model(i)
            if confidence >= wake_words[i].threshold {
                wake_words[i].detections = wake_words[i].detections + 1
                return 1
            }
        }
        i = i + 1
    }

    return 0
}

// Run wake word detection model (stub - would use actual model)
fn run_wake_word_model(model_idx: u32): u16 {
    // Would run neural network inference here
    // Returns confidence score 0-1000
    return 0
}

// Calculate audio frame energy
fn calculate_audio_energy(): u32 {
    var energy: u64 = 0
    var count: u32 = va_state.audio_buffer.sample_count

    if count == 0 { return 0 }
    if count > 480 { count = 480 }  // Last 30ms

    var start: u32 = 0
    if va_state.audio_buffer.sample_count > 480 {
        start = va_state.audio_buffer.sample_count - 480
    }

    var i: u32 = start
    while i < va_state.audio_buffer.sample_count {
        var sample: i32 = va_state.audio_buffer.samples[i]
        if sample < 0 { sample = -sample }
        energy = energy + sample
        i = i + 1
    }

    return energy / count
}

// Store audio samples in circular buffer
fn store_audio_samples(samples: u64, count: u32) {
    var i: u32 = 0
    while i < count {
        if va_state.audio_buffer.write_pos >= AUDIO_BUFFER_SIZE {
            va_state.audio_buffer.write_pos = 0
        }

        va_state.audio_buffer.samples[va_state.audio_buffer.write_pos] =
            @intToPtr(samples + (i * 2), i16)
        va_state.audio_buffer.write_pos = va_state.audio_buffer.write_pos + 1

        if va_state.audio_buffer.sample_count < AUDIO_BUFFER_SIZE {
            va_state.audio_buffer.sample_count = va_state.audio_buffer.sample_count + 1
        }

        i = i + 1
    }
}

// ============================================================================
// Speech Recording and Recognition
// ============================================================================

// Start recording user command
fn start_recording() {
    va_state.audio_buffer.is_recording = 1
    va_state.audio_buffer.write_pos = 0
    va_state.audio_buffer.sample_count = 0
    va_state.listen_start = foundation.timer_get_ticks()
}

// Stop recording and process speech
fn stop_recording() {
    va_state.audio_buffer.is_recording = 0
    va_state.state = VA_STATE_PROCESSING
    va_state.process_start = foundation.timer_get_ticks()
}

// Process recorded audio for speech recognition
export fn va_process_recording(): u32 {
    if va_state.state != VA_STATE_LISTENING { return 0 }

    var elapsed: u64 = foundation.timer_get_ticks() - va_state.listen_start

    // Check for timeout
    if elapsed >= va_state.config.listen_timeout_ms * 1000 {
        stop_recording()
        process_speech()
        return 1
    }

    // Check for end of speech (silence detection)
    var energy: u32 = calculate_audio_energy()
    if va_state.audio_buffer.vad_active == 1 {
        if energy < SILENCE_THRESHOLD {
            // Silence detected after speech
            var silence_duration: u64 = foundation.timer_get_ticks() - va_state.listen_start
            if silence_duration > 500000 {  // 500ms of silence
                stop_recording()
                process_speech()
                return 1
            }
        }
    } else {
        if energy >= SILENCE_THRESHOLD {
            va_state.audio_buffer.vad_active = 1
        }
    }

    return 0
}

// Process speech to text
fn process_speech() {
    foundation.serial_write_string("[Voice Assistant] Processing speech...\n")

    // Run speech recognition
    var result: SpeechResult
    recognize_speech(&result)

    if result.transcript_len > 0 {
        // Copy transcript to intent
        var i: u32 = 0
        while i < result.transcript_len {
            va_state.current_intent.raw_transcript[i] = result.transcript[i]
            i = i + 1
        }
        va_state.current_intent.transcript_len = result.transcript_len

        // Parse intent
        parse_intent()

        // Execute command
        execute_command()
    } else {
        // No speech detected
        respond("I didn't catch that. Could you please repeat?")
    }
}

// Speech recognition (stub - would use actual ASR engine)
fn recognize_speech(result: u64) {
    var r: SpeechResult = @intToPtr(result, SpeechResult)

    // Would run speech recognition model here
    // Options: Vosk, DeepSpeech, Whisper.cpp (all local)

    // For now, return empty - in real implementation:
    // 1. Preprocess audio (VAD, noise reduction)
    // 2. Extract features (MFCC, mel spectrogram)
    // 3. Run acoustic model
    // 4. Run language model decoding
    // 5. Return transcript with confidence

    r.transcript_len = 0
    r.confidence = 0
    r.is_final = 1
}

// ============================================================================
// Natural Language Understanding
// ============================================================================

// Parse user intent from transcript
fn parse_intent() {
    // Normalize transcript (lowercase, remove punctuation)
    normalize_transcript()

    // Extract entities
    extract_entities()

    // Match intent patterns
    match_intent_patterns()

    foundation.serial_write_string("[Voice Assistant] Intent: ")
    foundation.serial_write_hex(va_state.current_intent.intent)
    foundation.serial_write_string(", Action: ")
    foundation.serial_write_hex(va_state.current_intent.action)
    foundation.serial_write_string("\n")
}

// Normalize transcript for matching
fn normalize_transcript() {
    var i: u16 = 0
    while i < va_state.current_intent.transcript_len {
        var ch: u8 = va_state.current_intent.raw_transcript[i]

        // Convert to lowercase
        if ch >= 65 && ch <= 90 {
            va_state.current_intent.raw_transcript[i] = ch + 32
        }

        // Remove punctuation (except space)
        if ch < 32 || (ch > 32 && ch < 48) || (ch > 57 && ch < 65) ||
           (ch > 90 && ch < 97) || ch > 122 {
            if ch != 32 {
                va_state.current_intent.raw_transcript[i] = 32
            }
        }

        i = i + 1
    }
}

// Extract entities from transcript
fn extract_entities() {
    va_state.current_intent.entity_count = 0

    // Extract rooms
    extract_room_entities()

    // Extract devices
    extract_device_entities()

    // Extract numbers
    extract_number_entities()

    // Extract colors
    extract_color_entities()

    // Extract time/duration
    extract_time_entities()
}

// Extract room entities
fn extract_room_entities() {
    var i: u32 = 0
    while i < synonym_count {
        if synonyms[i].category == ENTITY_ROOM {
            var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                          &synonyms[i].word,
                                          va_state.current_intent.transcript_len)
            if pos >= 0 {
                add_entity(ENTITY_ROOM, &synonyms[i].canonical, pos, 90)
            }
        }
        i = i + 1
    }
}

// Extract device entities
fn extract_device_entities() {
    var i: u32 = 0
    while i < synonym_count {
        if synonyms[i].category == ENTITY_DEVICE {
            var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                          &synonyms[i].word,
                                          va_state.current_intent.transcript_len)
            if pos >= 0 {
                add_entity(ENTITY_DEVICE, &synonyms[i].canonical, pos, 90)
            }
        }
        i = i + 1
    }
}

// Extract number entities
fn extract_number_entities() {
    // Look for number words and digits
    var number_words: [u64; 10]
    number_words[0] = @ptrFromInt("zero")
    number_words[1] = @ptrFromInt("one")
    number_words[2] = @ptrFromInt("two")
    number_words[3] = @ptrFromInt("three")
    number_words[4] = @ptrFromInt("four")
    number_words[5] = @ptrFromInt("five")
    number_words[6] = @ptrFromInt("six")
    number_words[7] = @ptrFromInt("seven")
    number_words[8] = @ptrFromInt("eight")
    number_words[9] = @ptrFromInt("nine")

    // Check for number words
    var i: u32 = 0
    while i < 10 {
        var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                      number_words[i],
                                      va_state.current_intent.transcript_len)
        if pos >= 0 {
            var num_str: [u8; 4]
            num_str[0] = 48 + i
            num_str[1] = 0
            add_entity(ENTITY_NUMBER, &num_str, pos, 95)
        }
        i = i + 1
    }

    // Check for percentages (e.g., "50 percent")
    var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                  @ptrFromInt("percent"),
                                  va_state.current_intent.transcript_len)
    if pos > 0 {
        // Would extract number before "percent"
    }
}

// Extract color entities
fn extract_color_entities() {
    var i: u32 = 0
    while i < synonym_count {
        if synonyms[i].category == ENTITY_COLOR {
            var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                          &synonyms[i].word,
                                          va_state.current_intent.transcript_len)
            if pos >= 0 {
                add_entity(ENTITY_COLOR, &synonyms[i].canonical, pos, 90)
            }
        }
        i = i + 1
    }
}

// Extract time/duration entities
fn extract_time_entities() {
    // Look for time patterns: "in X minutes/hours", "at X o'clock"
    var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                  @ptrFromInt("minute"),
                                  va_state.current_intent.transcript_len)
    if pos > 0 {
        // Would extract number before "minute(s)"
        var duration_str: [u8; 8]
        copy_string(&duration_str, "60")  // Default 1 minute
        add_entity(ENTITY_DURATION, &duration_str, pos, 80)
    }

    pos = find_substring(&va_state.current_intent.raw_transcript,
                         @ptrFromInt("hour"),
                         va_state.current_intent.transcript_len)
    if pos > 0 {
        var duration_str: [u8; 8]
        copy_string(&duration_str, "3600")  // Default 1 hour
        add_entity(ENTITY_DURATION, &duration_str, pos, 80)
    }
}

// Add extracted entity
fn add_entity(entity_type: u8, value: u64, pos: i32, confidence: u8) {
    if va_state.current_intent.entity_count >= MAX_ENTITIES { return }

    var idx: u8 = va_state.current_intent.entity_count
    va_state.current_intent.entities[idx].entity_type = entity_type
    copy_string(&va_state.current_intent.entities[idx].value, value)
    va_state.current_intent.entities[idx].start_pos = pos
    va_state.current_intent.entities[idx].confidence = confidence

    va_state.current_intent.entity_count = va_state.current_intent.entity_count + 1
}

// Match intent patterns
fn match_intent_patterns() {
    va_state.current_intent.intent = INTENT_UNKNOWN
    va_state.current_intent.action = ACTION_QUERY
    va_state.current_intent.confidence = 0

    var best_confidence: u8 = 0

    var i: u32 = 0
    while i < pattern_count {
        if command_patterns[i].active == 1 {
            var confidence: u8 = match_pattern(i)
            if confidence > best_confidence {
                best_confidence = confidence
                va_state.current_intent.intent = command_patterns[i].intent
                va_state.current_intent.action = command_patterns[i].action
                va_state.current_intent.confidence = confidence
            }
        }
        i = i + 1
    }
}

// Match a single pattern
fn match_pattern(pattern_idx: u32): u8 {
    // Check if any pattern substring matches
    // Patterns are pipe-separated

    var pattern: u64 = @ptrFromInt(command_patterns[pattern_idx].patterns)
    var pos: i32 = find_substring(&va_state.current_intent.raw_transcript,
                                  pattern,
                                  va_state.current_intent.transcript_len)

    if pos >= 0 {
        return 80  // Base confidence for pattern match
    }

    return 0
}

// ============================================================================
// Command Execution
// ============================================================================

// Execute parsed command
fn execute_command() {
    va_state.commands_processed = va_state.commands_processed + 1

    var intent: u8 = va_state.current_intent.intent
    var action: u8 = va_state.current_intent.action

    if intent == INTENT_UNKNOWN {
        respond("I'm sorry, I didn't understand that command.")
        va_state.commands_failed = va_state.commands_failed + 1
        finish_response()
        return
    }

    // Check if confirmation is required
    if va_state.config.confirmation_required == 1 {
        if needs_confirmation(intent) {
            ask_confirmation()
            return
        }
    }

    // Execute based on intent
    if intent == INTENT_LIGHT_CONTROL {
        execute_light_command()
    } else if intent == INTENT_THERMOSTAT {
        execute_thermostat_command()
    } else if intent == INTENT_LOCK_CONTROL {
        execute_lock_command()
    } else if intent == INTENT_MEDIA_CONTROL {
        execute_media_command()
    } else if intent == INTENT_TIMER {
        execute_timer_command()
    } else if intent == INTENT_SCENE {
        execute_scene_command()
    } else if intent == INTENT_TIME {
        respond_time()
    } else if intent == INTENT_WEATHER {
        respond_weather()
    } else if intent == INTENT_DEVICE_STATUS {
        respond_device_status()
    } else if intent == INTENT_HELP {
        respond_help()
    } else if intent == INTENT_CANCEL {
        respond("Okay, cancelled.")
    } else {
        respond("I'm not sure how to do that yet.")
        va_state.commands_failed = va_state.commands_failed + 1
    }

    finish_response()
}

// Check if command needs confirmation
fn needs_confirmation(intent: u8): u8 {
    // Lock control and scene activation need confirmation
    if intent == INTENT_LOCK_CONTROL { return 1 }
    if intent == INTENT_SCENE { return 1 }
    return 0
}

// Ask for confirmation
fn ask_confirmation() {
    respond("Are you sure? Please confirm.")
    va_state.state = VA_STATE_LISTENING
    start_recording()
}

// Execute light control command
fn execute_light_command() {
    var room: [u8; 64]
    var device: [u8; 64]

    // Get room and device from entities
    get_entity_value(ENTITY_ROOM, &room)
    get_entity_value(ENTITY_DEVICE, &device)

    var action: u8 = va_state.current_intent.action

    if action == ACTION_ON {
        // Would call home automation API
        // home_auto.device_control(room, device, "on")
        respond("Turning on the lights.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_OFF {
        respond("Turning off the lights.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_SET {
        // Get brightness value
        var brightness: [u8; 8]
        get_entity_value(ENTITY_NUMBER, &brightness)
        respond("Setting brightness.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_TOGGLE {
        respond("Toggling the lights.")
        va_state.commands_successful = va_state.commands_successful + 1
    }
}

// Execute thermostat command
fn execute_thermostat_command() {
    var action: u8 = va_state.current_intent.action

    if action == ACTION_SET {
        var temp: [u8; 8]
        get_entity_value(ENTITY_NUMBER, &temp)
        respond("Setting temperature.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_INCREASE {
        respond("Increasing temperature.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_DECREASE {
        respond("Decreasing temperature.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_QUERY {
        respond("The current temperature is 72 degrees.")
        va_state.commands_successful = va_state.commands_successful + 1
    }
}

// Execute lock command
fn execute_lock_command() {
    var action: u8 = va_state.current_intent.action

    if action == ACTION_ON {
        respond("Locking the door.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_OFF {
        respond("Unlocking the door.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_QUERY {
        respond("The door is currently locked.")
        va_state.commands_successful = va_state.commands_successful + 1
    }
}

// Execute media command
fn execute_media_command() {
    var action: u8 = va_state.current_intent.action

    if action == ACTION_ON {
        respond("Playing media.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_OFF {
        respond("Pausing media.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_INCREASE {
        respond("Increasing volume.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_DECREASE {
        respond("Decreasing volume.")
        va_state.commands_successful = va_state.commands_successful + 1
    }
}

// Execute timer command
fn execute_timer_command() {
    var action: u8 = va_state.current_intent.action

    if action == ACTION_SET {
        var duration: [u8; 8]
        get_entity_value(ENTITY_DURATION, &duration)
        respond("Timer set.")
        va_state.commands_successful = va_state.commands_successful + 1
    } else if action == ACTION_OFF {
        respond("Timer cancelled.")
        va_state.commands_successful = va_state.commands_successful + 1
    }
}

// Execute scene command
fn execute_scene_command() {
    var scene: [u8; 64]
    get_entity_value(ENTITY_SCENE, &scene)
    respond("Activating scene.")
    va_state.commands_successful = va_state.commands_successful + 1
}

// Respond with current time
fn respond_time() {
    respond("The current time is displayed on your screen.")
    va_state.commands_successful = va_state.commands_successful + 1
}

// Respond with weather
fn respond_weather() {
    respond("Weather information is not available in offline mode.")
    va_state.commands_successful = va_state.commands_successful + 1
}

// Respond with device status
fn respond_device_status() {
    var device: [u8; 64]
    get_entity_value(ENTITY_DEVICE, &device)
    respond("Checking device status.")
    va_state.commands_successful = va_state.commands_successful + 1
}

// Respond with help
fn respond_help() {
    respond("I can control lights, thermostat, locks, and media. Try saying 'turn on the living room lights' or 'set temperature to 72 degrees'.")
    va_state.commands_successful = va_state.commands_successful + 1
}

// Get entity value by type
fn get_entity_value(entity_type: u8, value: u64) {
    var i: u8 = 0
    while i < va_state.current_intent.entity_count {
        if va_state.current_intent.entities[i].entity_type == entity_type {
            copy_string(value, &va_state.current_intent.entities[i].value)
            return
        }
        i = i + 1
    }

    // No entity found, set empty
    @ptrToInt(value, u8) = 0
}

// ============================================================================
// Text-to-Speech Response
// ============================================================================

// Generate spoken response
fn respond(text: u64) {
    foundation.serial_write_string("[Voice Assistant] Response: ")
    foundation.serial_write_string(text)
    foundation.serial_write_string("\n")

    // Copy to response buffer
    copy_string(&va_state.last_response, text)
    va_state.response_len = string_length(text)

    // Synthesize speech
    va_state.state = VA_STATE_RESPONDING
    synthesize_speech(text)
}

// Synthesize speech from text
fn synthesize_speech(text: u64) {
    // Would use TTS engine here
    // Options: eSpeak-ng, Piper TTS, VITS (all local)

    // TTS pipeline:
    // 1. Text normalization
    // 2. Grapheme-to-phoneme conversion
    // 3. Duration prediction
    // 4. Mel spectrogram generation
    // 5. Vocoder (waveform synthesis)

    // For now, just play a tone
    play_response_sound()
}

// Finish response and return to idle
fn finish_response() {
    // Calculate response time
    var response_time: u64 = foundation.timer_get_ticks() - va_state.wake_time
    var response_time_ms: u32 = response_time / 1000

    // Update average
    var total: u64 = va_state.avg_response_time_ms * (va_state.commands_processed - 1)
    va_state.avg_response_time_ms = (total + response_time_ms) / va_state.commands_processed

    // Play completion sound
    if va_state.config.feedback_sounds == 1 {
        play_done_sound()
    }

    // Return to idle
    va_state.state = VA_STATE_IDLE
    va_state.audio_buffer.vad_active = 0
}

// ============================================================================
// Audio Feedback
// ============================================================================

// Play wake word detection sound
fn play_wake_sound() {
    // Would play a short chime
}

// Play response sound
fn play_response_sound() {
    // Would play synthesized speech
}

// Play completion sound
fn play_done_sound() {
    // Would play a short tone
}

// ============================================================================
// Configuration
// ============================================================================

// Enable/disable voice assistant
export fn va_enable(enabled: u8) {
    va_state.config.enabled = enabled

    if enabled == 1 {
        va_state.state = VA_STATE_IDLE
    } else {
        va_state.state = VA_STATE_DISABLED
    }
}

// Set wake word
export fn va_set_wake_word(wake_word: u8) {
    va_state.config.wake_word = wake_word

    // Update enabled wake words
    var i: u32 = 0
    while i < MAX_WAKE_WORDS {
        wake_words[i].enabled = if i == wake_word { 1 } else { 0 }
        i = i + 1
    }
}

// Set custom wake word
export fn va_set_custom_wake_word(word: u64) {
    copy_string(&va_state.config.custom_wake_word, word)
    copy_string(&wake_words[WAKE_WORD_CUSTOM].keyword, word)
    wake_words[WAKE_WORD_CUSTOM].enabled = 1
    va_state.config.wake_word = WAKE_WORD_CUSTOM
}

// Set voice parameters
export fn va_set_voice(pitch: u8, speed: u8, volume: u8) {
    va_state.config.voice.pitch = pitch
    va_state.config.voice.speed = speed
    va_state.config.voice.volume = volume
}

// Set privacy mode
export fn va_set_privacy_mode(enabled: u8) {
    va_state.config.privacy_mode = enabled
}

// Set confirmation requirement
export fn va_set_confirmation(required: u8) {
    va_state.config.confirmation_required = required
}

// ============================================================================
// Query Functions
// ============================================================================

// Get current state
export fn va_get_state(): u8 {
    return va_state.state
}

// Get last response
export fn va_get_last_response(): u64 {
    return @ptrFromInt(va_state.last_response)
}

// Get statistics
export fn va_get_stats(wake_count: u64, cmd_count: u64, success_count: u64, avg_time: u64) {
    @ptrToInt(wake_count, u64) = va_state.wake_detections
    @ptrToInt(cmd_count, u64) = va_state.commands_processed
    @ptrToInt(success_count, u64) = va_state.commands_successful
    @ptrToInt(avg_time, u32) = va_state.avg_response_time_ms
}

// ============================================================================
// Utility Functions
// ============================================================================

// Copy string
fn copy_string(dest: u64, src: u64) {
    var i: u32 = 0
    while i < 256 {
        var ch: u8 = @intToPtr(src + i, u8)
        @ptrToInt(dest + i, u8) = ch
        if ch == 0 { break }
        i = i + 1
    }
}

// String length
fn string_length(s: u64): u16 {
    var len: u16 = 0
    while @intToPtr(s + len, u8) != 0 {
        len = len + 1
        if len >= 1024 { break }
    }
    return len
}

// Find substring (returns position or -1)
fn find_substring(haystack: u64, needle: u64, haystack_len: u16): i32 {
    var needle_len: u16 = string_length(needle)
    if needle_len == 0 { return -1 }
    if needle_len > haystack_len { return -1 }

    var i: u16 = 0
    while i <= haystack_len - needle_len {
        var match: u8 = 1
        var j: u16 = 0

        while j < needle_len {
            var h: u8 = @intToPtr(haystack + i + j, u8)
            var n: u8 = @intToPtr(needle + j, u8)

            if h != n {
                match = 0
                break
            }
            j = j + 1
        }

        if match == 1 {
            return i
        }

        i = i + 1
    }

    return -1
}

// ============================================================================
// Shutdown
// ============================================================================

export fn voice_assistant_shutdown() {
    if va_initialized == 0 { return }

    va_state.state = VA_STATE_DISABLED

    foundation.serial_write_string("[Voice Assistant] Statistics:\n")
    foundation.serial_write_string("  Wake detections: ")
    foundation.serial_write_hex(va_state.wake_detections)
    foundation.serial_write_string("\n  Commands processed: ")
    foundation.serial_write_hex(va_state.commands_processed)
    foundation.serial_write_string("\n  Commands successful: ")
    foundation.serial_write_hex(va_state.commands_successful)
    foundation.serial_write_string("\n  Avg response time: ")
    foundation.serial_write_hex(va_state.avg_response_time_ms)
    foundation.serial_write_string("ms\n")

    va_initialized = 0
    foundation.serial_write_string("[Voice Assistant] Shutdown complete\n")
}
